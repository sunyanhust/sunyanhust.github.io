<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://sunyanhust.github.io</id>
    <title>Gridea</title>
    <updated>2020-06-16T17:40:27.479Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://sunyanhust.github.io"/>
    <link rel="self" href="https://sunyanhust.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://sunyanhust.github.io/images/avatar.png</logo>
    <icon>https://sunyanhust.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, Gridea</rights>
    <entry>
        <title type="html"><![CDATA[LeetCode：有效的括号]]></title>
        <id>https://sunyanhust.github.io/post/leetcodeyou-xiao-de-gua-hao/</id>
        <link href="https://sunyanhust.github.io/post/leetcodeyou-xiao-de-gua-hao/">
        </link>
        <updated>2020-06-16T17:33:00.000Z</updated>
        <content type="html"><![CDATA[<h2 id="题目">题目</h2>
<p>给定一个只包括 '('，')'，'{'，'}'，'['，']' 的字符串，判断字符串是否有效。<br>
有效字符串需满足：</p>
<ol>
<li>左括号必须用相同类型的右括号闭合。</li>
<li>左括号必须以正确的顺序闭合。</li>
</ol>
<p>注意：空字符串可被认为是有效字符串。</p>
<p>示例 1:</p>
<pre><code class="language-bash">输入: &quot;()&quot;
输出: true
</code></pre>
<p>示例 2:</p>
<pre><code class="language-bash">输入: &quot;()[]{}&quot;
输出: true
</code></pre>
<p>示例 3:</p>
<pre><code class="language-bash">输入: &quot;(]&quot;
输出: false
</code></pre>
<p>示例 4:</p>
<pre><code class="language-bash">输入: &quot;([)]&quot;
输出: false
</code></pre>
<p>示例 5:</p>
<pre><code class="language-bash">输入: &quot;{[]}&quot;
输出: true
</code></pre>
<p>来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/valid-parentheses</p>
<h2 id="题解">题解</h2>
<p>运用栈的概念，先入后出。后进的左括号最先删除，最后判断栈的长度和某些特殊情况（如只有一个右括号的情况）。</p>
<pre><code class="language-python">class Solution:
    def isValid(self, s: str) -&gt; bool:
        
        if s==&quot;&quot;:
            return True
        
        temp = []
        for st in s:

            if st == &quot;(&quot; or st=='{' or st=='[':
                temp.append(st)
            else:
                
                if not temp: return False

                if st==')' and temp[-1]=='(':
                    temp.pop(-1)
                elif st=='}' and temp[-1]=='{':
                    temp.pop(-1)
                elif st==']' and temp[-1]=='[':
                    temp.pop(-1)
                else:
                    return False
        
        if len(temp)==0:
            return True
        else:
            return False
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[LeetCode：整数反转]]></title>
        <id>https://sunyanhust.github.io/post/leetcodezheng-shu-fan-zhuan/</id>
        <link href="https://sunyanhust.github.io/post/leetcodezheng-shu-fan-zhuan/">
        </link>
        <updated>2020-06-16T16:24:43.000Z</updated>
        <content type="html"><![CDATA[<h2 id="题目">题目</h2>
<p>给出一个 32 位的有符号整数，你需要将这个整数中每位上的数字进行反转。</p>
<p>示例 1:</p>
<pre><code class="language-bash">输入: 123
输出: 321
</code></pre>
<p>示例 2:</p>
<pre><code class="language-bash">输入: -123
输出: -321
</code></pre>
<p>示例 3:</p>
<pre><code class="language-bash">输入: 120
输出: 21
</code></pre>
<p>注意: 假设我们的环境只能存储得下 32 位的有符号整数，则其数值范围为 [−2^31,  2^31 − 1]。请根据这个假设，如果反转后整数溢出那么就返回 0。</p>
<p>来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/reverse-integer</p>
<h2 id="题解">题解</h2>
<p>基本做法有两种。一种是转字符串做，一种是用整数除法（空间上更优）。题目看的时候忘了注意部分，在提交查看错例的时候才猜测到，注意以后把题目看完。</p>
<h2 id="转为字符串">转为字符串</h2>
<pre><code class="language-python">class Solution:
    def reverse(self, x: int) -&gt; int:
        if x&gt;0:
            result = int(&quot;&quot;.join(list(str(x))[::-1]))
        else:
            result = -int(&quot;&quot;.join(list(str(-x))[::-1]))
        
        if -2**31&lt; result &lt;2**31-1:
            return result
        else:
            return 0
</code></pre>
<h2 id="整数除法">整数除法</h2>
<p>我们可以一次构建反转整数的一位数字。在这样做的时候，我们可以预先检查向原整数附加另一位数字是否会导致溢出。<br>
<img src="https://sunyanhust.github.io/post-images/1592325610163.png" alt="" loading="lazy"><br>
优化解：<br>
时间复杂度：O(log(x))，x中大约有log10(x) 位数字。<br>
空间复杂度：O(1)</p>
<pre><code class="language-python">class Solution:
    def reverse(self, x: int) -&gt; int:
        y, res = abs(x), 0
        boundry = 2**31-1 if x&gt;0 else 2**31
        while y != 0:
            res = res*10 +y%10
            if res &gt; boundry :
                return 0
            y //=10
        return res if x &gt;0 else -res
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[经典深度学习论文回顾：LeNet]]></title>
        <id>https://sunyanhust.github.io/post/jing-dian-shen-du-xue-xi-lun-wen-hui-gu-lenet/</id>
        <link href="https://sunyanhust.github.io/post/jing-dian-shen-du-xue-xi-lun-wen-hui-gu-lenet/">
        </link>
        <updated>2020-06-15T06:39:29.000Z</updated>
        <content type="html"><![CDATA[<h2 id="论文">论文</h2>
<p>论文：LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324.</p>
<h2 id="背景介绍">背景介绍</h2>
<p>Yan LeCun等人于1998年第一次将卷积神经网络（LeNet）应用到图像分类任务上，在手写数字识别任务上取得了巨大成功。LeNet的网络结构，麻雀虽小，但五脏俱全，卷积层、pooling层、全连接层，这些都是现代CNN网络的基本组件。</p>
<h2 id="模型结构">模型结构</h2>
<figure data-type="image" tabindex="1"><img src="https://sunyanhust.github.io/post-images/1592204054863.jpg" alt="" loading="lazy"></figure>
<h2 id="代码实现">代码实现</h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[LeetCode：最长公共前缀]]></title>
        <id>https://sunyanhust.github.io/post/leetcodezui-chang-gong-gong-qian-zhui/</id>
        <link href="https://sunyanhust.github.io/post/leetcodezui-chang-gong-gong-qian-zhui/">
        </link>
        <updated>2020-06-15T05:52:01.000Z</updated>
        <content type="html"><![CDATA[<h2 id="题目">题目</h2>
<p>编写一个函数来查找字符串数组中的最长公共前缀。如果不存在公共前缀，返回空字符串 &quot;&quot;。</p>
<p>示例 1:</p>
<pre><code class="language-bash">输入: [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;]
输出: &quot;fl&quot;
</code></pre>
<p>示例 2:</p>
<pre><code class="language-bash">输入: [&quot;dog&quot;,&quot;racecar&quot;,&quot;car&quot;]
输出: &quot;&quot;
解释: 输入不存在公共前缀。
</code></pre>
<p>说明: 所有输入只包含小写字母 a-z 。</p>
<p>来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/longest-common-prefix</p>
<h2 id="解法">解法</h2>
<p>方法1和方法2比较常规，我自己解题的时候用的方法2。方法3利用字符串排序，非常巧妙。方法4是字典树，直觉上可以，但是我对字典树不是很熟悉。</p>
<h3 id="横向扫描">横向扫描</h3>
<p>依次遍历字符串数组中的每个字符串，对于每个遍历到的字符串，更新最长公共前缀，当遍历完所有的字符串以后，即可得到字符串数组中的最长公共前缀。<br>
<img src="https://sunyanhust.github.io/post-images/1592200588023.png" alt="" loading="lazy"></p>
<h3 id="纵向扫描">纵向扫描</h3>
<p>纵向扫描时，从前往后遍历所有字符串的每一列，比较相同列上的字符是否相同，如果相同则继续对下一列进行比较，如果不相同则当前列不再属于公共前缀，当前列之前的部分为最长公共前缀。<br>
<img src="https://sunyanhust.github.io/post-images/1592200615614.png" alt="" loading="lazy"><br>
这种方法和我想的一致，贴一下我的代码：</p>
<pre><code class="language-python">class Solution:
    def longestCommonPrefix(self, strs: List[str]) -&gt; str:
        cp = &quot;&quot;
        if not strs:
            return cp
        min_str_len = min([len(s) for s in strs])
        strs_num = len(strs)
        for i in range(min_str_len):
            temp = strs[0][i]
            same = True
            for j in range(strs_num):
                if strs[j][i] != temp:
                    same =False
                    break
            if same:
                cp += temp
            else:
                return cp
        return cp
</code></pre>
<p>在题解中还看到一种使用zip函数的便捷写法：</p>
<pre><code class="language-python">class Solution(object):
    def longestCommonPrefix(self, strs):
        ans = ''
        for i in zip(*strs):
            if len(set(i)) == 1:
                ans += i[0]
            else:
                break
        return ans
</code></pre>
<p>利用python的zip函数，把str看成list然后把输入看成二维数组，左对齐纵向压缩，然后把每项利用集合去重，之后遍历list中找到元素长度大于1之前的就是公共前缀。</p>
<h3 id="字符串排序">字符串排序</h3>
<p>利用python的max()和min()，在python里字符串是可以比较的，按照ascII值排，举例abb， aba，abac，最大为abb，最小为aba。所以只需要比较最大最小的公共前缀就是整个数组的公共前缀。</p>
<pre><code class="language-python">def longestCommonPrefix(self, strs):
        if not strs: return &quot;&quot;
        s1 = min(strs)
        s2 = max(strs)
        for i,x in enumerate(s1):
            if x != s2[i]:
                return s2[:i]
        return s1
</code></pre>
<h3 id="字典树">字典树</h3>
<p>暂时不明白，后续理解一下。</p>
<pre><code class="language-python">from collections import defaultdict
from functools import reduce
class Solution:
    def longestCommonPrefix(self, strs: List[str]) -&gt; str:
        if not strs:return &quot;&quot;
        trieNode = lambda:defaultdict(trieNode)
        class Trie:
            def __init__(self):
                self.trie = trieNode()

            def insert(self,word):
                reduce(dict.__getitem__,word,self.trie)['END'] = True

            def search(self,word):
                reduce(lambda d,k:d[k] if k in d else trieNode(),word,self.trie).get('END',False)

            def startsWith(self,word):
                return len(reduce(lambda d,k:d[k] if k in d else trieNode(),word,self.trie)) &gt; 0

            def longestCommonPrefix(self):
                commonPrefixes = &quot;&quot;
                starts = self.trie
                while ('END' not in starts) and len(starts) == 1:
                    append = list(starts.keys())[0]
                    starts = starts.get(append)
                    commonPrefixes += append
                return commonPrefixes

        trie = Trie()
        for word in strs:
            trie.insert(word)
        return trie.longestCommonPrefix()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[如何入门深度学习]]></title>
        <id>https://sunyanhust.github.io/post/ru-he-ru-men-shen-du-xue-xi/</id>
        <link href="https://sunyanhust.github.io/post/ru-he-ru-men-shen-du-xue-xi/">
        </link>
        <updated>2020-05-22T10:20:45.000Z</updated>
        <content type="html"><![CDATA[<h2 id="step0搭建深度学习环境">Step0：搭建深度学习环境</h2>
<ul>
<li>Doker搭建深度学习环境：https://www.cnblogs.com/bingmang/p/9813686.html</li>
<li>Pycharm远程连接服务器：https://www.cnblogs.com/zhuminghui/p/10947930.html</li>
<li>服务器使用screen后台运行程序：https://sunyanhust.github.io/post/fu-wu-qi-shi-yong-screen-hou-tai-yun-xing-cheng-xu/</li>
</ul>
<h2 id="step1-通过阅读python深度学习掌握深度学习基础知识">Step1: 通过阅读《Python深度学习》掌握深度学习基础知识</h2>
<p>《Python深度学习》这本书是Keras之父Francois Chollet所著，该书假定读者无任何机器学习知识，以Keras为工具，使用丰富的范例示范深度学习的最佳实践，该书通俗易懂，全书没有一个数学公式，注重培养读者的深度学习直觉。</p>
<ul>
<li><strong>电子版下载</strong>：https://pan.baidu.com/s/1-4q6VjLTb3ZxcefyNCbjSA 提取码：wtzo，</li>
<li><strong>代码</strong>：https://github.com/fchollet/deep-learning-with-python-notebooks</li>
</ul>
<h2 id="step2通过教程30天吃掉那只-tensorflow2深入学习tensorflow">Step2：通过教程《30天吃掉那只 TensorFlow2》深入学习TensorFlow</h2>
<ul>
<li>📚 gitbook电子书地址： https://lyhue1991.github.io/eat_tensorflow2_in_30_days</li>
<li>🚀 github项目地址：https://github.com/lyhue1991/eat_tensorflow2_in_30_days</li>
<li>🐳 kesci专栏地址：https://www.kesci.com/home/column/5d8ef3c3037db3002d3aa3a0</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:right">日期</th>
<th style="text-align:left">学习内容</th>
<th style="text-align:right">内容难度</th>
<th style="text-align:right">预计学习时间</th>
<th style="text-align:right">更新状态</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E4%B8%80%E3%80%81TensorFlow%E7%9A%84%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B.md"><strong>一、TensorFlow的建模流程</strong></a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day1</td>
<td style="text-align:left"><a href="./1-1,%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B.md">1-1,结构化数据建模流程范例</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day2</td>
<td style="text-align:left"><a href="./1-2,%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B.md">1-2,图片数据建模流程范例</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day3</td>
<td style="text-align:left"><a href="./1-3,%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B.md">1-3,文本数据建模流程范例</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day4</td>
<td style="text-align:left"><a href="./1-4,%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B.md">1-4,时间序列数据建模流程范例</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E4%BA%8C%E3%80%81TensorFlow%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.md"><strong>二、TensorFlow的核心概念</strong></a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day5</td>
<td style="text-align:left"><a href="./2-1,%E5%BC%A0%E9%87%8F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.md">2-1,张量数据结构</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day6</td>
<td style="text-align:left"><a href="./2-2,%E4%B8%89%E7%A7%8D%E8%AE%A1%E7%AE%97%E5%9B%BE.md">2-2,三种计算图</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day7</td>
<td style="text-align:left"><a href="./2-3,%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E6%9C%BA%E5%88%B6.md">2-3,自动微分机制</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E4%B8%89%E3%80%81TensorFlow%E7%9A%84%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84.md"><strong>三、TensorFlow的层次结构</strong></a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day8</td>
<td style="text-align:left"><a href="./3-1,%E4%BD%8E%E9%98%B6API%E7%A4%BA%E8%8C%83.md">3-1,低阶API示范</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day9</td>
<td style="text-align:left"><a href="./3-2,%E4%B8%AD%E9%98%B6API%E7%A4%BA%E8%8C%83.md">3-2,中阶API示范</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day10</td>
<td style="text-align:left"><a href="./3-3,%E9%AB%98%E9%98%B6API%E7%A4%BA%E8%8C%83.md">3-3,高阶API示范</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E5%9B%9B%E3%80%81TensorFlow%E7%9A%84%E4%BD%8E%E9%98%B6API.md"><strong>四、TensorFlow的低阶API</strong></a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day11</td>
<td style="text-align:left"><a href="./4-1,%E5%BC%A0%E9%87%8F%E7%9A%84%E7%BB%93%E6%9E%84%E6%93%8D%E4%BD%9C.md">4-1,张量的结构操作</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day12</td>
<td style="text-align:left"><a href="./4-2,%E5%BC%A0%E9%87%8F%E7%9A%84%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97.md">4-2,张量的数学运算</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day13</td>
<td style="text-align:left"><a href="./4-3,AutoGraph%E7%9A%84%E4%BD%BF%E7%94%A8%E8%A7%84%E8%8C%83.md">4-3,AutoGraph的使用规范</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">0.5hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day14</td>
<td style="text-align:left"><a href="./4-4,AutoGraph%E7%9A%84%E6%9C%BA%E5%88%B6%E5%8E%9F%E7%90%86.md">4-4,AutoGraph的机制原理</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day15</td>
<td style="text-align:left"><a href="./4-5,AutoGraph%E5%92%8Ctf.Module.md">4-5,AutoGraph和tf.Module</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E4%BA%94%E3%80%81TensorFlow%E7%9A%84%E4%B8%AD%E9%98%B6API.md"><strong>五、TensorFlow的中阶API</strong></a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day16</td>
<td style="text-align:left"><a href="./5-1,%E6%95%B0%E6%8D%AE%E7%AE%A1%E9%81%93Dataset.md">5-1,数据管道Dataset</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day17</td>
<td style="text-align:left"><a href="./5-2,%E7%89%B9%E5%BE%81%E5%88%97feature_column.md">5-2,特征列feature_column</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day18</td>
<td style="text-align:left"><a href="./5-3,%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0activation.md">5-3,激活函数activation</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">0.5hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day19</td>
<td style="text-align:left"><a href="./5-4,%E6%A8%A1%E5%9E%8B%E5%B1%82layers.md">5-4,模型层layers</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day20</td>
<td style="text-align:left"><a href="./5-5,%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0losses.md">5-5,损失函数losses</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day21</td>
<td style="text-align:left"><a href="./5-6,%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87metrics.md">5-6,评估指标metrics</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day22</td>
<td style="text-align:left"><a href="./5-7,%E4%BC%98%E5%8C%96%E5%99%A8optimizers.md">5-7,优化器optimizers</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">0.5hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day23</td>
<td style="text-align:left"><a href="./5-8,%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0callbacks.md">5-8,回调函数callbacks</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E5%85%AD%E3%80%81TensorFlow%E7%9A%84%E9%AB%98%E9%98%B6API.md"><strong>六、TensorFlow的高阶API</strong></a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day24</td>
<td style="text-align:left"><a href="./6-1,%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B%E7%9A%843%E7%A7%8D%E6%96%B9%E6%B3%95.md">6-1,构建模型的3种方法</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day25</td>
<td style="text-align:left"><a href="./6-2,%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%843%E7%A7%8D%E6%96%B9%E6%B3%95.md">6-2,训练模型的3种方法</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day26</td>
<td style="text-align:left"><a href="./6-3,%E4%BD%BF%E7%94%A8%E5%8D%95GPU%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.md">6-3,使用单GPU训练模型</a></td>
<td style="text-align:right">⭐️⭐️</td>
<td style="text-align:right">0.5hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day27</td>
<td style="text-align:left"><a href="./6-4,%E4%BD%BF%E7%94%A8%E5%A4%9AGPU%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.md">6-4,使用多GPU训练模型</a></td>
<td style="text-align:right">⭐️⭐️</td>
<td style="text-align:right">0.5hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day28</td>
<td style="text-align:left"><a href="./6-5,%E4%BD%BF%E7%94%A8TPU%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.md">6-5,使用TPU训练模型</a></td>
<td style="text-align:right">⭐️⭐️</td>
<td style="text-align:right">0.5hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day29</td>
<td style="text-align:left"><a href="./6-6,%E4%BD%BF%E7%94%A8tensorflow-serving%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%9E%8B.md">6-6,使用tensorflow-serving部署模型</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day30</td>
<td style="text-align:left"><a href="./6-7,%E4%BD%BF%E7%94%A8spark-scala%E8%B0%83%E7%94%A8tensorflow%E6%A8%A1%E5%9E%8B.md">6-7,使用spark-scala调用tensorflow模型</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E5%90%8E%E8%AE%B0%EF%BC%9A%E4%B8%80%E4%B8%AA%E5%90%83%E8%B4%A7%E5%92%8C%E4%B8%80%E9%81%93%E8%8F%9C%E7%9A%84%E6%95%85%E4%BA%8B.md">后记：一个吃货和一道菜的故事</a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
</tbody>
</table>
<h2 id="step4-通过cnn-architectures项目复现常见cnn模型并阅读有关论文">Step4: 通过《CNN-Architectures》项目复现常见CNN模型，并阅读有关论文</h2>
<p><strong>CNN-Architectures：</strong>  https://github.com/Machine-Learning-Tokyo/CNN-Architectures/tree/master/Implementations</p>
<p>使用<code>tf.keras</code>API复现了一些常见CNN模型，包括：AlexNet、VGG、GoogLeNet、MobileNet、ResNet、Xception、SqueezeNet、DenseNet、ShuffleNet</p>
<h2 id="step5-通过deep-models-for-nlp-beginners项目学习nlp基础知识">Step5: 通过《Deep Models for NLP beginners》项目学习NLP基础知识</h2>
<p>Deep Models for NLP beginners：https://github.com/BrambleXu/nlp-beginner-guide-keras</p>
<p>包括词向量、情感分类以及实体识别</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[知识点备忘]]></title>
        <id>https://sunyanhust.github.io/post/zhi-shi-dian-bei-wang-he-todo/</id>
        <link href="https://sunyanhust.github.io/post/zhi-shi-dian-bei-wang-he-todo/">
        </link>
        <updated>2020-04-16T03:02:25.000Z</updated>
        <content type="html"><![CDATA[<h2 id="比较琐碎的知识">比较琐碎的知识</h2>
<ul>
<li>联想小新是开机按F2进入bios</li>
<li>windows上编辑的sh文件在linux上需要转换，转换软件为<code>doc2unix</code>，命令为<code>doc2unix filename</code></li>
<li>GPU机器之间拷贝文件直接scp加上机器的ip地址就可以，因为各个机器在同一局域网内</li>
</ul>
<h2 id="文本分类kaggle-kernel">文本分类kaggle kernel</h2>
<h3 id="基于lstm的多标签文本分类">基于LSTM的多标签文本分类</h3>
<p>kaggle kernel 链接： https://www.kaggle.com/rftexas/gru-lstm-rnn-101</p>
<p><strong>主要亮点</strong>：</p>
<ol>
<li>使用了tf.keras进行构建，很多代码可以复用为baseline</li>
<li>读取和加载Glove词向量</li>
<li>AUC作为评价标准</li>
<li>数据集处理为tf_dataset输入keras模型</li>
<li>在训练集训练后，在验证集继续训练两个epochs（小技巧，可能很有用）</li>
</ol>
<pre><code class="language-python">import gc
import pickle
import re
import string
import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from tensorflow.keras import backend as K
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras import initializers, regularizers, constraints
from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping
from tensorflow.keras.layers import Layer, Dense, Input, Embedding, SpatialDropout1D, Bidirectional, LSTM, \
    GlobalMaxPooling1D, GlobalAveragePooling1D
from tensorflow.keras.layers import concatenate
from tensorflow.keras.models import Model
from tqdm.notebook import tqdm

tqdm.pandas()

warnings.simplefilter('ignore')

# HYPERPARAMETERS
MAX_LEN = 220
MAX_FEATURES = 100000
EMBED_SIZE = 600
BATCH_SIZE = 128
N_EPOCHS = 5
LEARNING_RATE = 8e-4

# We will concatenate Crawl and GloVe embeddings
CRAWL_EMB_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'
GLOVE_EMB_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'


def display_training_curves(training, validation, title, subplot):
    &quot;&quot;&quot;
    Quickly display training curves
    &quot;&quot;&quot;
    if subplot % 10 == 1:
        plt.subplots(figsize=(10, 10), facecolor='#F0F0F0')
        plt.tight_layout()

    ax = plt.subplot(subplot)
    ax.set_facecolor('#F8F8F8')
    ax.plot(training)
    ax.plot(validation)
    ax.set_title('model' + title)
    ax.set_ylabel(title)
    ax.set_xlabel('epoch')
    ax.legend(['train', 'valid'])


def get_coeffs(word, *arr):
    return word, np.asarray(arr, dtype='float32')


def load_embeddings(embed_dir):
    with open(embed_dir, 'rb') as  infile:
        embeddings = pickle.load(infile)
        return embeddings


def build_embedding_matrix(word_index, embeddings_index, max_features, lower=True, verbose=True):
    embedding_matrix = np.zeros((max_features, 300))
    for word, i in tqdm(word_index.items(), len=(word_index.items())):
        if lower:
            word = word.lower()
        if i &gt;= max_features: continue
        try:
            embedding_vector = embeddings_index[word]
        except:
            embedding_vector = embeddings_index[&quot;unknown&quot;]
        if embedding_vector is not None:
            # words not found in embedding index will be all-zeros.
            embedding_matrix[i] = embedding_vector
    return embedding_matrix


def build_matrix(word_index, embeddings_index):
    embedding_matrix = np.zeros((len(word_index) + 1, 300))
    for word, i in word_index.items():
        try:
            embedding_matrix[i] = embeddings_index[word]
        except:
            embedding_matrix[i] = embeddings_index[&quot;unknown&quot;]
    return embedding_matrix


class Attention(Layer):
    &quot;&quot;&quot;
    Custom Keras attention layer
    Reference: https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043
    &quot;&quot;&quot;

    def __init__(self, step_dim, W_regularizer=None, b_regularizer=None,
                 W_constraint=None, b_constraint=None, bias=True, **kwargs):

        self.supports_masking = True

        self.bias = bias
        self.step_dim = step_dim
        self.features_dim = None
        super(Attention, self).__init__(**kwargs)

        self.param_W = {
            'initializer': initializers.get('glorot_uniform'),
            'name': '{}_W'.format(self.name),
            'regularizer': regularizers.get(W_regularizer),
            'constraint': constraints.get(W_constraint)
        }
        self.W = None

        self.param_b = {
            'initializer': 'zero',
            'name': '{}_b'.format(self.name),
            'regularizer': regularizers.get(b_regularizer),
            'constraint': constraints.get(b_constraint)
        }
        self.b = None

    def build(self, input_shape):
        assert len(input_shape) == 3

        self.features_dim = input_shape[-1]
        self.W = self.add_weight(shape=(input_shape[-1],),
                                 **self.param_W)

        if self.bias:
            self.b = self.add_weight(shape=(input_shape[1],),
                                     **self.param_b)

        self.built = True

    def compute_mask(self, input, input_mask=None):
        return None

    def call(self, x, mask=None):
        step_dim = self.step_dim
        features_dim = self.features_dim

        eij = K.reshape(
            K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))),
            (-1, step_dim))

        if self.bias:
            eij += self.b
        eij = K.tanh(eij)
        a = K.exp(eij)

        if mask is not None:
            a *= K.cast(mask, K.floatx())

        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())
        a = K.expand_dims(a)
        weighted_input = x * a
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0], self.features_dim


# We create a balanced

print('Loading train sets...')
train1 = pd.read_csv(&quot;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv&quot;)
train2 = pd.read_csv(&quot;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv&quot;)

train = pd.concat([
    train1[['comment_text', 'toxic']],
    train2[['comment_text', 'toxic']].query('toxic==1'),
    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)
])

del train1, train2

print('Loading validation sets...')
valid = pd.read_csv('/kaggle/input/val-en-df/validation_en.csv')

print('Loading test sets...')
test = pd.read_csv('/kaggle/input/test-en-df/test_en.csv')
sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')

misspell_dict = {&quot;aren't&quot;: &quot;are not&quot;, &quot;can't&quot;: &quot;cannot&quot;, &quot;couldn't&quot;: &quot;could not&quot;,
                 &quot;didn't&quot;: &quot;did not&quot;, &quot;doesn't&quot;: &quot;does not&quot;, &quot;don't&quot;: &quot;do not&quot;,
                 &quot;hadn't&quot;: &quot;had not&quot;, &quot;hasn't&quot;: &quot;has not&quot;, &quot;haven't&quot;: &quot;have not&quot;,
                 &quot;he'd&quot;: &quot;he would&quot;, &quot;he'll&quot;: &quot;he will&quot;, &quot;he's&quot;: &quot;he is&quot;,
                 &quot;i'd&quot;: &quot;I had&quot;, &quot;i'll&quot;: &quot;I will&quot;, &quot;i'm&quot;: &quot;I am&quot;, &quot;isn't&quot;: &quot;is not&quot;,
                 &quot;it's&quot;: &quot;it is&quot;, &quot;it'll&quot;: &quot;it will&quot;, &quot;i've&quot;: &quot;I have&quot;, &quot;let's&quot;: &quot;let us&quot;,
                 &quot;mightn't&quot;: &quot;might not&quot;, &quot;mustn't&quot;: &quot;must not&quot;, &quot;shan't&quot;: &quot;shall not&quot;,
                 &quot;she'd&quot;: &quot;she would&quot;, &quot;she'll&quot;: &quot;she will&quot;, &quot;she's&quot;: &quot;she is&quot;,
                 &quot;shouldn't&quot;: &quot;should not&quot;, &quot;that's&quot;: &quot;that is&quot;, &quot;there's&quot;: &quot;there is&quot;,
                 &quot;they'd&quot;: &quot;they would&quot;, &quot;they'll&quot;: &quot;they will&quot;, &quot;they're&quot;: &quot;they are&quot;,
                 &quot;they've&quot;: &quot;they have&quot;, &quot;we'd&quot;: &quot;we would&quot;, &quot;we're&quot;: &quot;we are&quot;,
                 &quot;weren't&quot;: &quot;were not&quot;, &quot;we've&quot;: &quot;we have&quot;, &quot;what'll&quot;: &quot;what will&quot;,
                 &quot;what're&quot;: &quot;what are&quot;, &quot;what's&quot;: &quot;what is&quot;, &quot;what've&quot;: &quot;what have&quot;,
                 &quot;where's&quot;: &quot;where is&quot;, &quot;who'd&quot;: &quot;who would&quot;, &quot;who'll&quot;: &quot;who will&quot;,
                 &quot;who're&quot;: &quot;who are&quot;, &quot;who's&quot;: &quot;who is&quot;, &quot;who've&quot;: &quot;who have&quot;,
                 &quot;won't&quot;: &quot;will not&quot;, &quot;wouldn't&quot;: &quot;would not&quot;, &quot;you'd&quot;: &quot;you would&quot;,
                 &quot;you'll&quot;: &quot;you will&quot;, &quot;you're&quot;: &quot;you are&quot;, &quot;you've&quot;: &quot;you have&quot;,
                 &quot;'re&quot;: &quot; are&quot;, &quot;wasn't&quot;: &quot;was not&quot;, &quot;we'll&quot;: &quot; will&quot;, &quot;tryin'&quot;: &quot;trying&quot;}


def _get_misspell(misspell_dict):
    misspell_re = re.compile('(%s)' % '|'.join(misspell_dict.keys()))
    return misspell_dict, misspell_re


def replace_typical_misspell(text):
    misspellings, misspellings_re = _get_misspell(misspell_dict)

    def replace(match):
        return misspellings[match.group(0)]

    return misspellings_re.sub(replace, text)


puncts = [',', '.', '&quot;', ':', ')', '(', '-', '!', '?', '|', ';', &quot;'&quot;, '$', '&amp;', '/', '[', ']',
          '&gt;', '%', '=', '#', '*', '+', '\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',
          '®', '`', '&lt;', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',
          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',
          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',
          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',
          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',
          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']


def clean_text(x):
    x = str(x)
    for punct in puncts + list(string.punctuation):
        if punct in x:
            x = x.replace(punct, f' {punct} ')
    return x


def clean_numbers(x):
    return re.sub(r'\d+', ' ', x)


def preprocess(train, valid, test, tfms):
    for tfm in tfms:
        print(tfm.__name__)
        train['comment_text'] = train['comment_text'].progress_apply(tfm)
        valid['comment_text_en'] = valid['comment_text_en'].progress_apply(tfm)
        test['content'] = test['content'].progress_apply(tfm)

    return train, valid, test


tfms = [replace_typical_misspell, clean_text, clean_numbers]
train, valid, test = preprocess(train, valid, test, tfms)

tokenizer = Tokenizer(num_words=MAX_FEATURES, filters='', lower=False)

print('Fitting tokenizer...')
tokenizer.fit_on_texts(list(train['comment_text']) + list(valid['comment_text_en']) + list(test['content_en']))
word_index = tokenizer.word_index

print('Building training set...')
X_train = tokenizer.texts_to_sequences(list(train['comment_text']))
y_train = train['toxic'].values

print('Building validation set...')
X_valid = tokenizer.texts_to_sequences(list(valid['comment_text_en']))
y_valid = valid['toxic'].values

print('Building test set ...')
X_test = tokenizer.texts_to_sequences(list(test['content_en']))

print('Padding sequences...')
X_train = pad_sequences(X_train, maxlen=MAX_LEN)
X_valid = pad_sequences(X_valid, maxlen=MAX_LEN)
X_test = pad_sequences(X_test, maxlen=MAX_LEN)

y_train = train.toxic.values
y_valid = valid.toxic.values

del tokenizer

print('Loading Crawl embeddings...')
crawl_embeddings = load_embeddings(CRAWL_EMB_PATH)

print('Loading GloVe embeddings...')
glove_embeddings = load_embeddings(GLOVE_EMB_PATH)

print('Building matrices...')
embedding_matrix_1 = build_matrix(word_index, crawl_embeddings)
embedding_matrix_2 = build_matrix(word_index, glove_embeddings)

print('Concatenating embedding matrices...')
embedding_matrix = np.concatenate([embedding_matrix_1, embedding_matrix_2], axis=1)

del embedding_matrix_1, embedding_matrix_2
del crawl_embeddings, glove_embeddings

gc.collect()

train_dataset = (
    tf.data.Dataset
        .from_tensor_slices((X_train, y_train))
        .repeat()
        .shuffle(2048)
        .batch(BATCH_SIZE)
)

valid_dataset = (
    tf.data.Dataset
        .from_tensor_slices((X_valid, y_valid))
        .batch(BATCH_SIZE)
        .cache()
)

test_dataset = (
    tf.data.Dataset
        .from_tensor_slices(X_test)
        .batch(BATCH_SIZE)
)


def build_model(word_index, embedding_matrix, verbose=True):
    &quot;&quot;&quot;
    credits go to: https://www.kaggle.com/thousandvoices/simple-lstm/
    &quot;&quot;&quot;
    sequence_input = Input(shape=(MAX_LEN,), dtype=tf.int32)

    embedding_layer = Embedding(*embedding_matrix.shape,
                                weights=[embedding_matrix],
                                trainable=False)

    x = embedding_layer(sequence_input)
    x = SpatialDropout1D(0.3)(x)
    x = Bidirectional(LSTM(256, return_sequences=True))(x)
    x = Bidirectional(LSTM(128, return_sequences=True))(x)

    att = Attention(MAX_LEN)(x)
    avg_pool1 = GlobalAveragePooling1D()(x)
    max_pool1 = GlobalMaxPooling1D()(x)
    hidden = concatenate([att, avg_pool1, max_pool1])

    hidden = Dense(512, activation='relu')(hidden)
    hidden = Dense(128, activation='relu')(hidden)
    out = Dense(1, activation='sigmoid')(hidden)
    model = Model(sequence_input, out)

    return model

model = build_model(word_index, embedding_matrix)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])
model.summary()

file_weights = 'best_model.h5'
# cb1 = ModelCheckpoint(file_weights, save_best_only=True)

cb2 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)
cb3 = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, cooldown=0, min_lr=0.0001)
cb4 = LearningRateScheduler(lambda epoch: LEARNING_RATE * (0.6 ** epoch))

n_steps = X_train.shape[0] // BATCH_SIZE

train_history = model.fit(
    train_dataset,
    steps_per_epoch=n_steps,
    validation_data=valid_dataset,
    callbacks=[cb4],
    epochs=N_EPOCHS
)

display_training_curves(
    train_history.history['loss'],
    train_history.history['val_loss'],
    'loss',
    211)

display_training_curves(
    train_history.history['auc'],
    train_history.history['val_auc'],
    'AUC',
    212)

n_steps = X_valid.shape[0] // BATCH_SIZE

train_history = model.fit(
    valid_dataset.repeat(),
    steps_per_epoch=n_steps,
    callbacks=[cb4],
    epochs=N_EPOCHS
)

preds = model.predict(test_dataset, verbose=1)
sub['toxic'] = preds

</code></pre>
<h3 id="基于bert的多标签文本分类使用tpu">基于BERT的多标签文本分类(使用TPU)</h3>
<p>kaggle kernel 链接： https://www.kaggle.com/sunyancn/jigsaw-tpu-bert-with-huggingface-and-keras</p>
<p><strong>主要亮点</strong>：</p>
<ol>
<li>使用了transformers的分词器进行快速分词</li>
<li>文本长度的可视化</li>
<li>TF Hub BERT模型的加载</li>
<li>TPU策略</li>
</ol>
<pre><code class="language-python"># %% [markdown]
# ## About this notebook
# 
# *[Jigsaw Multilingual Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification)* is the 3rd annual competition organized by the Jigsaw team. It follows *[Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)*, the original 2018 competition, and *[Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)*, which required the competitors to consider biased ML predictions in their new models. This year, the goal is to use english only training data to run toxicity predictions on many different languages, which can be done using multilingual models, and speed up using TPUs.
# 
# Many awesome notebooks has already been made so far. Many of them used really cool technologies like [Pytorch XLA](https://www.kaggle.com/theoviel/bert-pytorch-huggingface-starter). This notebook instead aims at constructing a **fast, concise, reusable, and beginner-friendly model scaffold**. It will focus on the following points:
# * **Using Tensorflow and Keras**: Tensorflow is a powerful framework, and Keras makes the training process extremely easy to understand. This is especially good for beginners to learn how to use TPUs, and for experts to focus on the modelling aspect.
# * **Using Huggingface's `transformers` library**: [This library](https://huggingface.co/transformers/) is extremely popular, so using this let you easily integrate the end result into your ML pipelines, and can be easily reused for your other projects.
# * **Native TPU usage**: The TPU usage is abstracted using the native `strategy` that was created using Tensorflow's `tf.distribute.experimental.TPUStrategy`. This avoids getting too much into the lower-level aspect of TPU management.
# * **Use a subset of the data**: Instead of using the entire dataset, we will only use the 2018 subset of the data available, which makes this much faster, all while achieving a respectable accuracy.

# %% [code]
import os
import warnings

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from kaggle_datasets import KaggleDatasets
import transformers
import traitlets
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm
from tokenizers import BertWordPieceTokenizer
from sklearn.metrics import roc_auc_score

warnings.simplefilter(&quot;ignore&quot;)

# %% [markdown]
# ## Helper Functions

# %% [code]
def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):
    tokenizer.enable_truncation(max_length=maxlen)
    tokenizer.enable_padding(max_length=maxlen)
    all_ids = []
    
    for i in tqdm(range(0, len(texts), chunk_size)):
        text_chunk = texts[i:i+chunk_size].tolist()
        encs = tokenizer.encode_batch(text_chunk)
        all_ids.extend([enc.ids for enc in encs])
    
    return np.array(all_ids)

# %% [code]
def build_model(transformer, loss='binary_crossentropy', max_len=512):
    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;)
    sequence_output = transformer(input_word_ids)[0]
    cls_token = sequence_output[:, 0, :]
    x = tf.keras.layers.Dropout(0.35)(cls_token)
    out = Dense(1, activation='sigmoid')(x)
    
    model = Model(inputs=input_word_ids, outputs=out)
    model.compile(Adam(lr=3e-5), loss=loss, metrics=[tf.keras.metrics.AUC()])
    
    return model

# %% [markdown]
# Cosine similarity calculates similarity by measuring the cosine of angle between two vectors. This is calculated as:
# ![](https://miro.medium.com/max/426/1*hub04IikybZIBkSEcEOtGA.png)
# 
# Cosine Similarity calculation for two vectors A and B [source]
# With cosine similarity, we need to convert sentences into vectors. One way to do that is to use bag of words with either TF (term frequency) or TF-IDF (term frequency- inverse document frequency). The choice of TF or TF-IDF depends on application and is immaterial to how cosine similarity is actually performed — which just needs vectors. TF is good for text similarity in general, but TF-IDF is good for search query relevance.

# %% [code]
# https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents
import nltk, string
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('punkt') # if necessary...


stemmer = nltk.stem.porter.PorterStemmer()
remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)

def stem_tokens(tokens):
    return [stemmer.stem(item) for item in tokens]

'''remove punctuation, lowercase, stem'''
def normalize(text):
    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))

vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')

def cosine_sim(text1, text2):
    tfidf = vectorizer.fit_transform([text1, text2])
    return ((tfidf * tfidf.T).A)[0,1]

# %% [markdown]
# ## TPU Configs

# %% [code]
AUTO = tf.data.experimental.AUTOTUNE

# Create strategy from tpu
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
strategy = tf.distribute.experimental.TPUStrategy(tpu)

# Data access
#GCS_DS_PATH = KaggleDatasets().get_gcs_path('kaggle/input/') 

# %% [markdown]
# ## Create fast tokenizer

# %% [code]
# First load the real tokenizer
tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')

# Save the loaded tokenizer locally
save_path = '/kaggle/working/distilbert_base_uncased/'
if not os.path.exists(save_path):
    os.makedirs(save_path)
tokenizer.save_pretrained(save_path)

# Reload it with the huggingface tokenizers library
fast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased/vocab.txt', lowercase=True)
fast_tokenizer

# %% [markdown]
# ## Load text data into memory

# %% [code]
train1 = pd.read_csv(&quot;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv&quot;)
train2 = pd.read_csv(&quot;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv&quot;)

valid = pd.read_csv('/kaggle/input/val-en-df/validation_en.csv')
test1 = pd.read_csv('/kaggle/input/test-en-df/test_en.csv')
test2 = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_test_translated.csv')
sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')

# %% [code]
test2.head()

# %% [markdown]
# ## Test dataset comparision

# %% [code]
plt.figure(figsize=(12, 8))

sns.distplot(train1.comment_text.str.len(), label='train')
sns.distplot(test1.content_en.str.len(), label='test1')
sns.distplot(test2.translated.str.len(), label='test2')
plt.legend();

# %% [code]
plt.figure(figsize=(12, 8))

sns.distplot(train1.comment_text.str.len(), label='train')
sns.distplot(test1.content_en.str.len(), label='test1')
sns.distplot(test2.translated.str.len(), label='test2')
plt.xlim([0, 512])
plt.legend();

# %% [markdown]
# Lets calculate cosine similarity two translated test datasets.

# %% [code]
test_set_similarity = [cosine_sim(t1, t2) for t1, t2 in tqdm(zip(test1.content_en, test2.translated))]

plt.figure(figsize=(12, 8))

sns.distplot(test_set_similarity);

# %% [markdown]
# ## Fast encode

# %% [code]
x_train = fast_encode(train1.comment_text.astype(str), fast_tokenizer, maxlen=512)
x_valid = fast_encode(valid.comment_text_en.astype(str), fast_tokenizer, maxlen=512)
x_test1 = fast_encode(test1.content_en.astype(str), fast_tokenizer, maxlen=512)
x_test2 = fast_encode(test2.translated.astype(str), fast_tokenizer, maxlen=512)

y_train = train1.toxic.values
y_valid = valid.toxic.values

# %% [markdown]
# ## Build datasets objects

# %% [code]
train_dataset = (
    tf.data.Dataset
    .from_tensor_slices((x_train, y_train))
    .repeat()
    .shuffle(2048)
    .batch(64)
    .prefetch(AUTO)
)

valid_dataset = (
    tf.data.Dataset
    .from_tensor_slices((x_valid, y_valid))
    .batch(64)
    .cache()
    .prefetch(AUTO)
)

test_dataset = [(
    tf.data.Dataset
    .from_tensor_slices(x_test1)
    .batch(64)
),
    (
    tf.data.Dataset
    .from_tensor_slices(x_test2)
    .batch(64)
)]

# %% [markdown]
# # Focal Loss

# %% [code]
from tensorflow.keras import backend as K

def focal_loss(gamma=2., alpha=.2):
    def focal_loss_fixed(y_true, y_pred):
        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))
        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))
        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))
    return focal_loss_fixed

# %% [markdown]
# ## Load model into the TPU

# %% [code]
%%time
with strategy.scope():
    transformer_layer = transformers.TFBertModel.from_pretrained('bert-base-uncased')
    model = build_model(transformer_layer, loss=focal_loss(gamma=1.5), max_len=512)
model.summary()

# %% [markdown]
# ## RocAuc Callback

# %% [code]
from tensorflow.keras.callbacks import Callback 

class RocAucCallback(Callback):
    def __init__(self, test_data, score_thr):
        self.test_data = test_data
        self.score_thr = score_thr
        self.test_pred = []
        
    def on_epoch_end(self, epoch, logs=None):
        if logs['val_auc'] &gt; self.score_thr:
            print('\nRun TTA...')
            for td in self.test_data:
                self.test_pred.append(self.model.predict(td))

# %% [markdown]
# # LrScheduler

# %% [code]
def build_lrfn(lr_start=0.000001, lr_max=0.000002, 
               lr_min=0.0000001, lr_rampup_epochs=7, 
               lr_sustain_epochs=0, lr_exp_decay=.87):
    lr_max = lr_max * strategy.num_replicas_in_sync

    def lrfn(epoch):
        if epoch &lt; lr_rampup_epochs:
            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start
        elif epoch &lt; lr_rampup_epochs + lr_sustain_epochs:
            lr = lr_max
        else:
            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min
        return lr
    
    return lrfn

# %% [code]
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 7))

_lrfn = build_lrfn()
plt.plot([i for i in range(35)], [_lrfn(i) for i in range(35)]);

# %% [markdown]
# ## Train Model

# %% [code]
roc_auc = RocAucCallback(test_dataset, 0.9195)
lrfn = build_lrfn()
lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)

train_history = model.fit(
    train_dataset,
    steps_per_epoch=150,
    validation_data=valid_dataset,
    callbacks=[lr_schedule, roc_auc],
    epochs=35
)

# %% [markdown]
# ## Submission

# %% [code]
sub['toxic'] = np.mean(roc_auc.test_pred, axis=0)
sub.to_csv('submission.csv', index=False)

# %% [markdown]
# # Reference
# * [Jigsaw TPU: DistilBERT with Huggingface and Keras](https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras)
# * [inference of bert tpu model ml w/ validation](https://www.kaggle.com/abhishek/inference-of-bert-tpu-model-ml-w-validation)
# * [Overview of Text Similarity Metrics in Python](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50)
# * [test-en-df](https://www.kaggle.com/bamps53/test-en-df)
# * [val_en_df](https://www.kaggle.com/bamps53/val-en-df)
# * [Jigsaw multilingual toxic - test translated](https://www.kaggle.com/kashnitsky/jigsaw-multilingual-toxic-test-translated)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latex画神经网络图]]></title>
        <id>https://sunyanhust.github.io/post/latex-hua-shen-jing-wang-luo-tu/</id>
        <link href="https://sunyanhust.github.io/post/latex-hua-shen-jing-wang-luo-tu/">
        </link>
        <updated>2020-04-12T03:56:10.000Z</updated>
        <content type="html"><![CDATA[<h2 id="bilstm">BiLSTM</h2>
<h3 id="效果图">🥙效果图</h3>
<figure data-type="image" tabindex="1"><img src="https://sunyanhust.github.io/post-images/1587243946926.png" alt="" loading="lazy"></figure>
<h3 id="代码">📜代码</h3>
<pre><code class="language-tex">\documentclass[crop, tikz]{standalone}
\usepackage{tikz}

\usetikzlibrary{positioning}

\begin{document}
\begin{tikzpicture}
	\node[rectangle] (Y0) at (0, 0) {$\dots$};
	\node[rectangle, draw, right=2em of Y0, minimum height=1cm, minimum width=1cm] (RNN) {LSTM$_\rightarrow$};
	\node[rectangle, right=of RNN, draw, minimum height=1cm, minimum width=1cm] (RNN2) {LSTM$_\rightarrow$};
	\node[rectangle, right=of RNN2, draw, minimum height=1cm, minimum width=1cm] (RNN3) {LSTM$_\rightarrow$};
			
	\node[rectangle, right= of RNN3, draw, minimum height=1cm, minimum width=1cm] (RNN4) {LSTM$_\rightarrow$};
	\node[rectangle, right=2em of RNN4] (RNN5) {$\dots$};
			
			
	\node[rectangle, above=of RNN4, draw, minimum height=1cm, minimum width=1cm] (R25) {LSTM$_\leftarrow$};
	\node[rectangle, left=of R25, minimum height=1cm, minimum width=1cm, draw] (R24) {LSTM$_\leftarrow$};
	\node[rectangle, left=of R24, draw, minimum height=1cm, minimum width=1cm] (R23) {LSTM$_\leftarrow$};
	\node[rectangle, left=of R23, draw, minimum height=1cm, minimum width=1cm] (R22) {LSTM$_\leftarrow$};
	\node[rectangle, left=2em of R22] (R21) {$\dots$};
	\node[right=2em of R25] (Y20) {$\dots$};
			
	\node[below=of RNN] (X1) {$\vec{x}_1$};
	\node[below=of RNN2] (X2) {$\vec{x}_2$};
	\node[below=of RNN3] (X3) {$\vec{x}_3$};
	\node[below=of RNN4] (X4) {$\vec{x}_4$};
	\node[above=of R25] (Y5) {$\vec{h}_4$};
	\node[above=of R24] (Y4) {$\vec{h}_3$};
	\node[above=of R23] (Y3) {$\vec{h}_2$};
	\node[above=of R22] (Y2) {$\vec{h}_1$};
			
	\draw[-stealth, thick] (X1) -- (RNN);
	\draw[-stealth, thick] (X2) -- (RNN2);
	\draw[-stealth, thick] (X3) -- (RNN3);
	\draw[-stealth, thick] (X4) -- (RNN4);
	\draw[-stealth, thick, densely dotted] (Y0) -- (RNN);
	\draw[-stealth, thick] (RNN) -- node[above, pos=0.35] {$\vec{h}_2^\rightarrow$} (RNN2);
	\draw[-stealth, thick] (RNN2) -- node[above, pos=0.35] {$\vec{h}_3^\rightarrow$} (RNN3);
	\draw[-stealth, thick] (RNN3) -- node[above, pos=0.35] {$\vec{h}_4^\rightarrow$} (RNN4);
	\draw[-stealth, densely dotted, thick] (RNN4) -- (RNN5);
	\node[below=4em of Y0] (d) {\dots};
	\node[below=4em of RNN5] (d) {\dots};
			
	\path[-stealth, ultra thick, white] (X1) edge[bend left=45] (R22);
	\path[-stealth, thick] (X1) edge[bend left=45] (R22);
	\path[-stealth, ultra thick, white] (X2) edge[bend left=45] (R23);
	\path[-stealth, thick] (X2) edge[bend left=45] (R23);
	\path[-stealth, ultra thick, white] (X3) edge[bend left=45] (R24);
	\path[-stealth, thick] (X3) edge[bend left=45] (R24);
	\path[-stealth, ultra thick, white] (X4) edge[bend left=45] (R25);
	\path[-stealth, thick] (X4) edge[bend left=45] (R25);
	\draw[-stealth, densely dotted, thick] (Y20) -- (R25);
			
	\draw[-stealth, thick] (R22) -- (Y2);
	\draw[-stealth, thick] (R23) -- (Y3);
	\draw[-stealth, thick] (R24) -- (Y4);
	\draw[-stealth, thick] (R25) -- (Y5);
		
	\draw[stealth-, densely dotted, thick] (R21) -- (R22);
	\draw[stealth-, thick] (R22) -- node[above, pos=0.65] {$\vec{h}_3^\leftarrow$} (R23);
	\draw[stealth-, thick] (R23) -- node[above, pos=0.65] {$\vec{h}_4^\leftarrow$} (R24);
	\draw[stealth-, thick] (R24) -- node[above, pos=0.65] {$\vec{h}_5^\leftarrow$} (R25);
	\draw[-stealth, densely dotted, thick] (Y20) -- (R25);	
			
	\path[-stealth, ultra thick, white] (RNN) edge[bend right=45] (Y2);
	\path[-stealth, thick] (RNN) edge[bend right=45] (Y2);
	\path[-stealth, ultra thick, white] (RNN2) edge[bend right=45] (Y3);
	\path[-stealth, thick] (RNN2) edge[bend right=45] (Y3);
	\path[-stealth, ultra thick, white] (RNN3) edge[bend right=45] (Y4);
	\path[-stealth, thick] (RNN3) edge[bend right=45] (Y4);
	\path[-stealth, ultra thick, white] (RNN4) edge[bend right=45] (Y5);
	\path[-stealth, thick] (RNN4) edge[bend right=45] (Y5);
			
\end{tikzpicture}
\end{document}
</code></pre>
<h2 id="lstm单元">LSTM单元</h2>
<h3 id="效果图-2">🥙效果图</h3>
<figure data-type="image" tabindex="2"><img src="https://sunyanhust.github.io/post-images/1587241071902.PNG" alt="" loading="lazy"></figure>
<h3 id="代码-2">📜代码</h3>
<pre><code class="language-tex">\documentclass[crop, tikz]{standalone}
\usepackage{tikz}

\usepackage{bm}
\usepackage{relsize}
\usepackage{pgfplots}
 
\usetikzlibrary{arrows,shapes, decorations.pathmorphing,backgrounds,positioning}

\begin{document}
\begin{tikzpicture}

	\node[rectangle, rounded corners=10, minimum width=20em, minimum height=12em, draw, very thick, fill=white] (lstm) at (0, 0) {};
	
	\node[rectangle, draw] at (-2.5, -0.8) (s1){$\sigma$};
	\node[rectangle, draw, right=1em of s1] (s2) {$\sigma$};
	\node[rectangle, draw, right=1em of s2] (t1) {$tanh$};
	\node[rectangle, draw, right=1em of t1] (s3) {$\sigma$};
	\node[circle, draw, above=2em of t1, inner sep=0em] (m1) {$\otimes$};
	\node[circle, draw, above=6em of s1, inner sep=0em] (m2) {$\otimes$};
	\node[circle, draw, right=6.55em of m2, inner sep=0em] (p1) {$\oplus$};
	\node[circle, draw, right=4.5em of m1, inner sep=0em] (m3) {$\otimes$};
	\node[rectangle, draw, above=1em of m3, inner sep=0.2em] (tt) {$tanh$ };
	
	\node[circle, draw, below=1em of s1, inner sep=0em] (conc) {$||$};
	
	\node[below=5em of s1] (xt) {$\vec{x}_t$};
	\node[left=3em of conc] (ht1) {$\vec{h}_{t-1}$};
	\node[left=3em of m2] (ct1) {$c_{t-1}$};
	\node[right=18em of m2] (ct) {$c_t$};
	\node[right=18em of conc] (ht) {$\vec{h}_t$};
	\node[] (yt) at (3, 3) {$\vec{h}_t$};
	
	\draw[-stealth, line width=1mm, white] (xt) -- (conc);
	\draw[-stealth, very thick] (xt) -- (conc);
	\draw[-stealth, line width=1mm, white] (ht1) -- (conc);
	\draw[-stealth, very thick] (ht1) -- (conc);
	
	\draw[-stealth, very thick] (conc) -- (s1);
	\path[-stealth, very thick] (conc) edge[bend right] (s2.south);
	\path[-stealth, very thick] (conc) edge[bend right] (t1.south);
	\path[-stealth, very thick] (conc) edge[bend right] (s3.south);
	\draw[-stealth, very thick] (s1) -- node[left] {$f_t$} (m2);
	\draw[-stealth, very thick] (s2) edge[bend left] node[above] {$i_t$} (m1.west);
	\draw[-stealth, very thick] (t1) -- node[right] {$\tilde{c_t}$} (m1);
	\draw[-stealth, very thick] (m1) -- (p1);
	\draw[-stealth, line width=1mm, white] (ct1) -- (m2);
	\draw[-stealth, very thick] (ct1) -- (m2);
	\draw[-stealth, very thick] (m2) -- (p1);
	\draw[-stealth, very thick] (s3) edge[bend left] node[left] {$o_t$} (m3.west);
	
	\draw[-stealth, line width=1mm, white] (p1) -- (ct);
	\draw[-stealth, very thick] (p1) -- (ct);
	\draw[-stealth, very thick] (tt) -- (m3);
	\draw[-stealth, line width=1mm, white] (m3) edge[bend right] (ht.west);
	\draw[-stealth, very thick] (m3) edge[bend right] (ht.west);
	
	\draw[-stealth ,very thick] (p1) edge[bend right] (tt.west);
	\draw[-stealth, line width=1mm,white] (m3) edge[bend right] (yt.south);
	\draw[-stealth, very thick] (m3) edge[bend right] (yt.south);
			
\end{tikzpicture}
\end{document}
</code></pre>
<h2 id="自注意力">自注意力</h2>
<h3 id="效果图-3">🥙效果图</h3>
<figure data-type="image" tabindex="3"><img src="https://sunyanhust.github.io/post-images/1587241576659.png" alt="" loading="lazy"></figure>
<h3 id="代码-3">📜代码</h3>
<pre><code class="language-tex">\documentclass[crop, tikz]{standalone}
\usepackage{tikz}

\usetikzlibrary{positioning}

\begin{document}
\begin{tikzpicture}

	\node (X1) {$\vec{h}_{1}$};

	\node[rectangle, right= 0.5em of X1] (x_dots_1) {$\dots$};

	\node[right=0.5em of x_dots_1] (Xj) {$\vec{h}_{j}$};

	\node[rectangle, right= 1em of Xj] (x_dots_2) {$\dots$};

	\node[right=1em of x_dots_2] (Xn) {$\vec{h}_{n}$};

	\node[rectangle, draw, ultra thick, above=of X1] (attn1) {\large $a_\phi$};

	\node[rectangle, draw, ultra thick, above=of Xj] (attnj) {\large $a_\phi$};

	\node[rectangle, draw, ultra thick, above=of Xn] (attnn) {\large $a_\phi$};


	\draw[-stealth, thick] (X1) -- (attn1);
	\draw[-stealth, thick] (Xj) -- (attn1);

	\draw[-stealth, thick] (Xj) -- (attnj);
	\draw[-stealth, thick] ([xshift=3em]Xj) -- (attnj);
	
	\draw[-stealth, thick] (Xj) -- (attnn);
	\draw[-stealth, thick] (Xn) -- (attnn);
	
	\node[above= of attn1, opacity=0.2] (alpha1j) {$\alpha_{1,j}$};
	\node[above= of attnj, opacity=1] (alphajj) {$\alpha_{j,j}$};
	\node[above= of attnn, opacity=0.6] (alphanj) {$\alpha_{n,j}$};
	
	\node[circle, draw, above=of alpha1j] (times1) {$\times$};
	\node[circle, draw, above=of alphajj] (timesj) {$\times$};
	\node[circle, draw, above=of alphanj] (timesn) {$\times$};
	
	\node[rectangle, draw, above=of timesj] (sum) {$\Sigma$};

	\node[above=1em of sum] (x_tprim) {$\vec{h}_j'$};

	\draw[-stealth, line width=1.5mm, white] (attn1) -- (alpha1j);
	\draw[-stealth, thick, opacity=0.2] (attn1) -- (alpha1j);
	\draw[-stealth, line width=1.5mm, white] (attnj) -- (alphajj);
	\draw[-stealth, thick, opacity=1] (attnj) -- (alphajj);
	\draw[-stealth, line width=1.5mm, white] (attnn) -- (alphanj);
	\draw[-stealth, thick, opacity=0.6] (attnn) -- (alphanj);
	
	\draw[-stealth, white, line width=1.5mm] (X1) edge[bend right=30] (times1);
	\draw[-stealth, thick] (X1) edge[bend right=30] node[rectangle, draw, fill=white, midway] {$f_\psi$} (times1);
	\draw[-stealth, white, line width=1.5mm] (Xj) edge[bend right=30] (timesj);
	\draw[-stealth, thick] (Xj) edge[bend right=30] node[rectangle, draw, fill=white, midway] {$f_\psi$} (timesj);
	\draw[-stealth, thick] (Xn) edge[bend right=30] node[rectangle, draw, fill=white, midway] {$f_\psi$} (timesn);

	\draw[-, line width=1.5mm, white] (times1) -- (sum);
	\draw[-stealth, thick] (times1) -- (sum);
	\draw[-, line width=1.5mm, white] (timesj) -- (sum);
	\draw[-stealth, thick] (timesj) -- (sum);
	\draw[-stealth, thick] (timesn) -- (sum);
	\draw[-stealth, thick] (times1) -- (sum);
		
	\draw[-stealth, line width=1.5mm, white] (alpha1j) -- (times1);
	\draw[-stealth, thick, opacity=0.2] (alpha1j) -- (times1);
	\draw[-stealth, line width=1.5mm, white] (alphajj) -- (timesj);
	\draw[-stealth, thick, opacity=1] (alphajj) -- (timesj);
	\draw[-stealth, line width=1.5mm, white] (alphanj) -- (timesn);
	\draw[-stealth, thick, opacity=0.6] (alphanj) -- (timesn);

	\draw[-stealth, thick] (sum) -- (x_tprim);

\end{tikzpicture}
\end{document}
</code></pre>
<h2 id="rnn">RNN</h2>
<h3 id="效果图-4">🥙效果图</h3>
<figure data-type="image" tabindex="4"><img src="https://sunyanhust.github.io/post-images/1587243725699.png" alt="" loading="lazy"></figure>
<h3 id="代码-4">📜代码</h3>
<pre><code class="language-tex">\documentclass[crop, tikz]{standalone}
\usepackage{tikz}

\usetikzlibrary{positioning}

\begin{document}
\begin{tikzpicture}
	\node[rectangle] (Y0) at (0, 0) {$\dots$};
	\node[rectangle, draw, right=2em of Y0, minimum height=1cm, minimum width=1cm] (RNN) {RNN};
	\node[rectangle, right=of RNN, draw, minimum height=1cm, minimum width=1cm] (RNN2) {RNN};
	\node[rectangle, right=of RNN2, draw, minimum height=1cm, minimum width=1cm] (RNN3) {RNN};
	\node[rectangle, right= of RNN3, draw, minimum height=1cm, minimum width=1cm] (RNN4) {RNN};
	\node[rectangle, right=2em of RNN4] (RNN5) {$\dots$};
			
	\node[below=of RNN] (X1) {$\vec{x}_1$};
	\node[below=of RNN2] (X2) {$\vec{x}_2$};
	\node[below=of RNN3] (X3) {$\vec{x}_3$};
	\node[below=of RNN4] (X4) {$\vec{x}_4$};
	\node[above=of RNN4] (Y5) {$\vec{h}_4$};
	\node[above=of RNN3] (Y4) {$\vec{h}_3$};
	\node[above=of RNN2] (Y3) {$\vec{h}_2$};
	\node[above=of RNN] (Y2) {$\vec{h}_1$};
			
	\draw[-stealth, thick] (X1) -- (RNN);
	\draw[-stealth, thick] (X2) -- (RNN2);
	\draw[-stealth, thick] (X3) -- (RNN3);
	\draw[-stealth, thick] (X4) -- (RNN4);
	
	\draw[-stealth, thick] (RNN) -- (Y2);
	\draw[-stealth, thick] (RNN2) -- (Y3);
	\draw[-stealth, thick] (RNN3) -- (Y4);
	\draw[-stealth, thick] (RNN4) -- (Y5);
	
    \draw[-stealth, densely dotted, thick] (Y0) -- (RNN);
    \draw[-stealth, densely dotted, thick] (RNN) -- (RNN2);
    \draw[-stealth, densely dotted, thick] (RNN2) -- (RNN3);
    \draw[-stealth, densely dotted, thick] (RNN3) -- (RNN4);
    \draw[-stealth, densely dotted, thick] (RNN4) -- (RNN5);
			
\end{tikzpicture}
\end{document}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PaddleHub使用示例]]></title>
        <id>https://sunyanhust.github.io/post/paddlehub/</id>
        <link href="https://sunyanhust.github.io/post/paddlehub/">
        </link>
        <updated>2020-04-12T03:04:18.000Z</updated>
        <content type="html"><![CDATA[<p>最近用了一下PaddleHub，感觉还挺好用的。这里两个使用PaddleHub的示例。</p>
<h2 id="分词">分词</h2>
<p>这个分词和官网的分词效果一样，觉得比jieba之类的要好。</p>
<pre><code class="language-python"># pip install pyahocorasick
# https://www.paddlepaddle.org.cn/hubdetail?name=lac&amp;en_category=LexicalAnalysis
import paddlehub as hub

temp_user_dict = [
    dict(word='自然', tag='n', freq='10000')
]


def make_dict(user_dicts):
    with open('user.dict', 'w') as f:
        for user_dict in user_dicts:
            f.write(user_dict['word'] + '\t' +
                    user_dict['tag'] + '\t' +
                    user_dict['freq'] + '\n')


make_dict(temp_user_dict)

lac = hub.Module(name='lac')
lac.set_user_dict(dict_path='user.dict')
results = lac.lexical_analysis(texts=['我爱自然语言处理'],
                               use_gpu=False,
                               batch_size=1,
                               return_tag=True)

for result in results:
    print(result[&quot;word&quot;])
    print(result[&quot;tag&quot;])
</code></pre>
<h2 id="阅读理解">阅读理解</h2>
<pre><code class="language-python">import paddlehub as hub

module = hub.Module(name=&quot;roberta_wwm_ext_chinese_L-24_H-1024_A-16&quot;)
inputs, outputs, program = module.context(trainable=True, max_seq_len=384)
dataset = hub.dataset.CMRC2018()

reader = hub.reader.ReadingComprehensionReader(
    dataset=dataset,
    vocab_path=module.get_vocab_path(),
    max_seq_len=384)

strategy = hub.AdamWeightDecayStrategy(
    learning_rate=5e-5,
    weight_decay=0.01,
    warmup_proportion=0.1
)

config = hub.RunConfig(use_cuda=False, num_epoch=2, batch_size=12, strategy=strategy)
seq_output = outputs[&quot;sequence_output&quot;]

# feed_list的Tensor顺序不可以调整
feed_list = [
    inputs[&quot;input_ids&quot;].name,
    inputs[&quot;position_ids&quot;].name,
    inputs[&quot;segment_ids&quot;].name,
    inputs[&quot;input_mask&quot;].name,
]

reading_comprehension_task = hub.ReadingComprehensionTask(
    data_reader=reader,
    feature=seq_output,
    feed_list=feed_list,
    config=config,
    sub_task=&quot;cmrc2018&quot;)

reading_comprehension_task.finetune_and_eval()

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[对待婚姻的态度]]></title>
        <id>https://sunyanhust.github.io/post/guan-yu-hun-yin/</id>
        <link href="https://sunyanhust.github.io/post/guan-yu-hun-yin/">
        </link>
        <updated>2020-04-06T13:42:02.000Z</updated>
        <content type="html"><![CDATA[<p>今天晚上，得知我最好的朋友离婚了。孩子才1岁，同样身为父亲的我感到难过，离婚对孩子来说影响太大。</p>
<p>我没资格说这些。不过，至少我的婚姻算凑合，我也有把握延续这种凑合。我写这篇文章，就是希望有更多的人能把握自己的婚姻。就算我说的都是废话，如果能引起朋友们在婚前对婚姻多一些思考，就达到目的了。</p>
<p>每个人对婚姻的态度都是不同的，随便找个人过日子的，找个女人帮自己生孩的，找个过夜不收钱的，这些所谓的“婚姻”就不在讨论范围了，这里和大家讨论的，是高质量的婚姻。</p>
<p>每个人的理解都不一样，在我看来，婚姻的状态有很多种，但是让人舒服的婚姻，一定都是有爱的。</p>
<p>有人说，爱情是有保质期的。我说，有保质期的不叫爱情。那叫激情，激情夹杂的东西太多，性欲，感动，内疚，憧憬，有太多太多的杂质，这样的情感确实难以持久。何况，激情往往是精心呵护起来的，一旦丢失了精心呵护的动力，褪色太快。</p>
<p>泡妞的时候，激情是最好的工具。然而面临结婚选择时，作为男人，一定要理清自己的头脑，祛除激情的成分。这个思考的过程非常重要，婚姻是没有回头路的。别以为大不了还可以离婚。离婚不是解脱，是又一个麻烦的开始。</p>
<p>如何确定自己爱不爱一个女人，这是非常关键的一步。我很肯定的说，很少男人清楚这个问题。男人的生理天性决定了对女人的选择很大因素是外貌。但是婚姻是反人类天性的。所以当男人选择了婚姻，就注定要克服自己的生理天性。</p>
<p>男人的生理天性，说白就是希望保质保量的遗传自己的基因，比如尽可能的和更多女人上床，尽可能的选择更漂亮，基因更优秀的女人上床。</p>
<p>而婚姻，恰恰相反，男人只能选择一个女人。而且要知道，眼前这个将要嫁给自己的美女，不几年就会生孩子，眼角会有皱纹，乳房会下垂，乳头会变的很黑，屁股会变形，小肚子会出来，还会有难看的斑纹。更可怕的是，男人的生理天性决定了男人老和同一个女人上床，是会腻的。所以，没有爱的婚姻是危险的。因为没有克服生理欲望的信念。</p>
<p>​       兄弟们永远别在婚前信誓旦旦的说自己不是那种包二奶的人。50年代出身那批人，道德观念比我们重的多，老婆还都是共过患难的糟糠，不一样大量的出轨？所以，在一夫一妻的制度下，保障婚姻的根本，还是得依靠爱情。其他约束在人性本能的拉扯下都是脆弱的。</p>
<p>据我观察，很多失败婚姻的元凶就是性。性对男人的诱惑是致命的，不少男人就是因为和女人上了床就顺理成章的结婚了。是好是坏全看运气。压根不思考有没有共同语言，遇到问题双方的沟通方式彼此能否接受，这些关键的问题不去想。就因为上了床就结婚。这是很多悲剧的来源。用人性本能来适应反人性本能的婚姻制度。这是非常愚蠢的。更愚蠢的是很多女人以性为工具来对待婚姻。说到这里才发现，婚姻真是个大话题。每个人去领结婚证的时候，都没想过有一天会去拿离婚证。可惜，离婚的人越来越多。据统计80后的离婚率快到百分四十了。</p>
<p>男人，只有把性欲，感动等等等等因素全抛开，才能真正认识自己对一个女人的感情是不是爱。爱不能解决所有问题，但是能给男人解决问题的动力。没有动力经营的婚姻，麻烦很多。因为婚姻的问题很多。</p>
<p>结婚对男人而言，纯粹是责任。有了承担责任的决心，才敢谈婚姻，所以，结婚一定要找一个自己爱的女人。自己余下的生命全部为之付出的家庭，女主人不是自己真正爱的，太悲剧了。因为婚姻唯一能回报给男人的，仅仅是一只能牵着自己走向死亡的手。千万别奢望婚姻回报给自己什么，任何奢望只会添加双方的压力。做好丈夫该做的，自然得到应得的。</p>
<p>很重要的一点，不是平时有没有共同语言，而是有分歧和问题的沟通。说白了，如果男人是个讲道理的人，女人也得是个懂道理的人，如果男人是个喜欢用拳头说话的人，女人也得是个肯挨拳头的人。女人如果喜欢唠叨，男人就得听得惯。总之，两个人必须得有个拿出统一意见的程序，且双方都能接受。最好是都乐意接受。其实这一条对于真正相爱的人来讲不是问题，真正相爱的两个人永远是站在对方立场思考问题的。</p>
<p>有情饮水饱，只能饱一顿。男人没结婚，是潇洒的，一结婚，负担就来了。两个人的结合所带的压力肯定是巨大的，必须要考虑双方的承受能力，自己没有能力负担，就不要害人害己了。女朋友如果喜欢钱，有钱就娶，没钱就不要砸锅卖铁娶回家了。</p>
<p>有些负担，背上就是一辈子的。对婚姻而言，肯陪自己一起奋斗的女人，才真正应该珍惜。单单是两个人一起改善生活的过程，就已经是婚姻宝贵的财富了。别说现在这样的女人少，不少。老盯着花枝招展想靠嫁人改变命运的年轻女人，就别怪女人现实。自己从未规划过未来，就别怪女人不肯陪自己一起奋斗。</p>
<p>我一直提倡婚前性行为，性生活得到满足的男人，才更容易发现女人其他的魅力。而现实的情况是，不少好女人还剩着，男人却在追逐性的过程中迷失了自己。因色而结合，女人色衰之后，没本事的男人继续鬼混，有本事的换个女人凑合。这样的风气还有个很烦的影响，女人越来越在意自己的形象，化妆时间越来越长，衣领越来越低，裙子越来越短。而充电的时间越来越少，独立意识越来越淡。</p>
<p>男人一边追逐女色，一边抱怨女人素质越来越低。</p>
<p>女人呢？一边抱怨男人肤浅，一边不断迎合肤浅的审美。</p>
<p>婚姻不是人生的全部，两个人携手一生，相助则利，相阻则损。性格互补也好，志同道合也好，彼此成为对方的助力，真的很重要。如果和一个女人婚前就感觉疲惫。相信我，别结婚了。家是港湾，是一个让男人快到家门就会不自觉加快脚步的地方。是一个一回家就会彻底放松，卸下所有伪装的地方。是一个所有笑容都发自真心的地方。</p>
<p>怎样区分激情和爱情？其实很好区分的，当你不见她时茶饭不思，是激情。老想见到她，想和她一起玩，一起上床，一起做白日梦，是激情。每天短信电话多的没完，是激情。为了生日节日费尽心思想浪漫的点子，是激情。</p>
<p>当你和她在一起时脑子里不自觉的规划实实在在的将来，是爱情。当你们争吵到很凶，火很大时，也不忍心说一句伤害她的话，是爱情。当你们有分歧时，你总是能清楚的知道她是怎么想的，能理解她的初衷，是爱情。</p>
<p>现在很多人谈恋爱很短时间就结婚了，当然，不是说时间短就不好，而是要清楚的知道，婚前的考察对这一生的影响。失败的家庭是任何成功都弥补不了的。</p>
<p>人，一旦离过一次婚，再寻找幸福的难度就更大了。因为，离过婚的人更难相信爱情。更何况离婚对子女的影响，太大太大。人，真正能留在这个世界上的，也就是子女这点血脉而已。为子女营造一个好的成长环境，是父亲的责任。</p>
<p>前面说这么多其实都是说面对婚姻的态度要理性，要慎重。</p>
<p>态度端正以后，技巧也有很多需要注意的地方。明明相爱的两个人互相伤害的事情太多了。如果犯错，也要积极想办法补救争取。伤寒心散伙的也多，而且婚姻生活，男人还有个重要技术要掌握，就是老妈与老婆的关系。婚后生活琐碎的事情太多，先说说处理婆媳关系。所有还没结婚的兄弟一定不要把这不当一回事。一边是生你养你为你付出所有的母亲，一边是将要相伴你一生的老婆。双方都有责任要照顾。这个关系弄顺了，少很多麻烦。</p>
<p>有人说男人在婆媳之间是双面胶，夹在中间两面讨好。这是错的。男人必须是一手拿棒一手拿润滑油。小事居中调节，原则问题对事不对人，道理绝不能歪，既不能纵容媳妇，也不能惯坏了老妈。坚决不要在老妈面前说媳妇不对，多包涵之类，也不能在媳妇面前说，这都是纵容。把握一个原则，绝不允许媳妇在自己面前说老妈的坏话，有事说事。该怎么解决怎么解决，是老妈不对也要和老妈把事情讲清楚，同样的，老妈说自己媳妇的时候也要分清是非。居中讨好的后果是两面不是人，矛盾还越来越深。</p>
<p>总之，绝对的公平是处理婆媳关系的唯一方式，委屈任何一方，都会使矛盾激化。妈也好，老婆也好，底线都给她们画好。人都是有选择性的，既然要不到特权，自然会注意彼此相处。一开始可能男人日子不太好过，两边都要斗上几次。但是慢慢的，家庭秩序就会走上正轨。 如果一开始图省心，哄过去，两边脾气都养大了就够自己受了。（前面忘了说了，找女人一定要找个聪明的，笨女人多很多麻烦，聪明的女人自己会处理，自己只用打打下手，关系就处的很好了。）</p>
<p>还有个问题，就是子女，现在的孩子才真叫一个宝贝，一大家人围着转，各有各的主意，很多矛盾都是因此而起，在这一点上，男人一定要强硬，从一开始就绝对不能让步，要给出明确的信号，孩子是自己的。双方父母肯帮忙，感谢。但是涉及孩子的一切决定。必须是自己拿主意。（这一点背地里可以多听老婆的，毕竟，妈妈是最爱孩子的。）这一点申明尤其重要，相当于自己一个人把所有炮火揽了。否则，孩子一点点小感冒家里可以闹翻天。（我那今天离婚的朋友就是因为这个）。</p>
<p>婚姻确实是个大话题，一时反而不知道说什么好了，我是真心想和大家交流一下。因为我自认为自己的婚姻真的是种幸福。我是真心希望有更多的人能愉快的享受婚姻。有人说不吵架不算夫妻，说真的，我和我老婆真还吵不起来架，就像我前面说的，伤彼此的话确实不忍心讲出口。何况一旦清楚彼此都是出于爱，又有什么好吵的？有几次刚进入状态，看见彼此装腔作势生气的样子就都笑了。</p>
<p>512地震的时候，我在震区，这么大的城市，通讯中断的情况下，我第一时间见到了我的老婆，不在我们上班的附近，也不在我们家附近，而是在我父母的家门口。她知道体谅我担心父母的心情，我知道她能体谅我。这就是婚姻的默契。也是婚姻和恋人的区别，婚姻承载的更多。</p>
<p>恋人时刻只有甜蜜和浪漫，而婚姻则更多是责任和平淡，这个转化的过程，是需要双方有充足思想准备的。其实只要两个人肯一起面对，婚姻生活平淡中的幸福并不输给热恋时的浪漫。</p>
<blockquote>
<p>转载在<a href="https://www.zhihu.com/question/19732277/answer/1056367198">知乎</a></p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[nvidia-smi指令报错：Failed to initialize NVML: Driver解决]]></title>
        <id>https://sunyanhust.github.io/post/nvidia-smi-zhi-ling-bao-cuo-failed-to-initialize-nvml-driver-jie-jue/</id>
        <link href="https://sunyanhust.github.io/post/nvidia-smi-zhi-ling-bao-cuo-failed-to-initialize-nvml-driver-jie-jue/">
        </link>
        <updated>2020-04-05T15:18:44.000Z</updated>
        <content type="html"><![CDATA[<p>最近装深度学习环境的时候遇见的，主要原因是旧的显卡驱动没卸载。</p>
<p>首先需要卸载驱动</p>
<pre><code class="language-shell">sudo apt-get purge nvidia*
</code></pre>
<p>然后重新安装新的驱动即可。</p>
<p>参考文章：https://www.zhihu.com/people/sun-yan-90-29</p>
]]></content>
    </entry>
</feed>