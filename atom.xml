<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://sunyanhust.github.io</id>
    <title>Gridea</title>
    <updated>2020-06-15T06:17:28.813Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://sunyanhust.github.io"/>
    <link rel="self" href="https://sunyanhust.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://sunyanhust.github.io/images/avatar.png</logo>
    <icon>https://sunyanhust.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, Gridea</rights>
    <entry>
        <title type="html"><![CDATA[LeetCode：最长公共前缀]]></title>
        <id>https://sunyanhust.github.io/post/leetcodezui-chang-gong-gong-qian-zhui/</id>
        <link href="https://sunyanhust.github.io/post/leetcodezui-chang-gong-gong-qian-zhui/">
        </link>
        <updated>2020-06-15T05:52:01.000Z</updated>
        <content type="html"><![CDATA[<h2 id="题目">题目</h2>
<p>编写一个函数来查找字符串数组中的最长公共前缀。如果不存在公共前缀，返回空字符串 &quot;&quot;。</p>
<p>示例 1:</p>
<pre><code class="language-bash">输入: [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;]
输出: &quot;fl&quot;
</code></pre>
<p>示例 2:</p>
<pre><code class="language-bash">输入: [&quot;dog&quot;,&quot;racecar&quot;,&quot;car&quot;]
输出: &quot;&quot;
解释: 输入不存在公共前缀。
</code></pre>
<p>说明: 所有输入只包含小写字母 a-z 。</p>
<p>来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/longest-common-prefix</p>
<h2 id="解法">解法</h2>
<p>方法1和方法2比较常规，我自己解题的时候用的方法2。方法3利用字符串排序，非常巧妙。方法4是字典树，直觉上可以，但是我对字典树不是很熟悉。</p>
<h3 id="横向扫描">横向扫描</h3>
<p>依次遍历字符串数组中的每个字符串，对于每个遍历到的字符串，更新最长公共前缀，当遍历完所有的字符串以后，即可得到字符串数组中的最长公共前缀。<br>
<img src="https://sunyanhust.github.io/post-images/1592200588023.png" alt="" loading="lazy"></p>
<h3 id="纵向扫描">纵向扫描</h3>
<p>纵向扫描时，从前往后遍历所有字符串的每一列，比较相同列上的字符是否相同，如果相同则继续对下一列进行比较，如果不相同则当前列不再属于公共前缀，当前列之前的部分为最长公共前缀。<br>
<img src="https://sunyanhust.github.io/post-images/1592200615614.png" alt="" loading="lazy"><br>
这种方法和我想的一致，贴一下我的代码：</p>
<pre><code class="language-python">class Solution:
    def longestCommonPrefix(self, strs: List[str]) -&gt; str:
        cp = &quot;&quot;
        if not strs:
            return cp
        min_str_len = min([len(s) for s in strs])
        strs_num = len(strs)
        for i in range(min_str_len):
            temp = strs[0][i]
            same = True
            for j in range(strs_num):
                if strs[j][i] != temp:
                    same =False
                    break
            if same:
                cp += temp
            else:
                return cp
        return cp
</code></pre>
<p>在题解中还看到一种使用zip函数的便捷写法：</p>
<pre><code class="language-python">class Solution(object):
    def longestCommonPrefix(self, strs):
        ans = ''
        for i in zip(*strs):
            if len(set(i)) == 1:
                ans += i[0]
            else:
                break
        return ans
</code></pre>
<p>利用python的zip函数，把str看成list然后把输入看成二维数组，左对齐纵向压缩，然后把每项利用集合去重，之后遍历list中找到元素长度大于1之前的就是公共前缀。</p>
<h3 id="字符串排序">字符串排序</h3>
<p>利用python的max()和min()，在python里字符串是可以比较的，按照ascII值排，举例abb， aba，abac，最大为abb，最小为aba。所以只需要比较最大最小的公共前缀就是整个数组的公共前缀。</p>
<pre><code class="language-python">def longestCommonPrefix(self, strs):
        if not strs: return &quot;&quot;
        s1 = min(strs)
        s2 = max(strs)
        for i,x in enumerate(s1):
            if x != s2[i]:
                return s2[:i]
        return s1
</code></pre>
<h3 id="字典树">字典树</h3>
<p>暂时不明白，后续理解一下。</p>
<pre><code class="language-python">from collections import defaultdict
from functools import reduce
class Solution:
    def longestCommonPrefix(self, strs: List[str]) -&gt; str:
        if not strs:return &quot;&quot;
        trieNode = lambda:defaultdict(trieNode)
        class Trie:
            def __init__(self):
                self.trie = trieNode()

            def insert(self,word):
                reduce(dict.__getitem__,word,self.trie)['END'] = True

            def search(self,word):
                reduce(lambda d,k:d[k] if k in d else trieNode(),word,self.trie).get('END',False)

            def startsWith(self,word):
                return len(reduce(lambda d,k:d[k] if k in d else trieNode(),word,self.trie)) &gt; 0

            def longestCommonPrefix(self):
                commonPrefixes = &quot;&quot;
                starts = self.trie
                while ('END' not in starts) and len(starts) == 1:
                    append = list(starts.keys())[0]
                    starts = starts.get(append)
                    commonPrefixes += append
                return commonPrefixes

        trie = Trie()
        for word in strs:
            trie.insert(word)
        return trie.longestCommonPrefix()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[如何入门深度学习]]></title>
        <id>https://sunyanhust.github.io/post/ru-he-ru-men-shen-du-xue-xi/</id>
        <link href="https://sunyanhust.github.io/post/ru-he-ru-men-shen-du-xue-xi/">
        </link>
        <updated>2020-05-22T10:20:45.000Z</updated>
        <content type="html"><![CDATA[<h2 id="step0搭建深度学习环境">Step0：搭建深度学习环境</h2>
<ul>
<li>Doker搭建深度学习环境：https://www.cnblogs.com/bingmang/p/9813686.html</li>
<li>Pycharm远程连接服务器：https://www.cnblogs.com/zhuminghui/p/10947930.html</li>
<li>服务器使用screen后台运行程序：https://sunyanhust.github.io/post/fu-wu-qi-shi-yong-screen-hou-tai-yun-xing-cheng-xu/</li>
</ul>
<h2 id="step1-通过阅读python深度学习掌握深度学习基础知识">Step1: 通过阅读《Python深度学习》掌握深度学习基础知识</h2>
<p>《Python深度学习》这本书是Keras之父Francois Chollet所著，该书假定读者无任何机器学习知识，以Keras为工具，使用丰富的范例示范深度学习的最佳实践，该书通俗易懂，全书没有一个数学公式，注重培养读者的深度学习直觉。</p>
<ul>
<li><strong>电子版下载</strong>：https://pan.baidu.com/s/1-4q6VjLTb3ZxcefyNCbjSA 提取码：wtzo，</li>
<li><strong>代码</strong>：https://github.com/fchollet/deep-learning-with-python-notebooks</li>
</ul>
<h2 id="step2通过教程30天吃掉那只-tensorflow2深入学习tensorflow">Step2：通过教程《30天吃掉那只 TensorFlow2》深入学习TensorFlow</h2>
<ul>
<li>📚 gitbook电子书地址： https://lyhue1991.github.io/eat_tensorflow2_in_30_days</li>
<li>🚀 github项目地址：https://github.com/lyhue1991/eat_tensorflow2_in_30_days</li>
<li>🐳 kesci专栏地址：https://www.kesci.com/home/column/5d8ef3c3037db3002d3aa3a0</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:right">日期</th>
<th style="text-align:left">学习内容</th>
<th style="text-align:right">内容难度</th>
<th style="text-align:right">预计学习时间</th>
<th style="text-align:right">更新状态</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E4%B8%80%E3%80%81TensorFlow%E7%9A%84%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B.md"><strong>一、TensorFlow的建模流程</strong></a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day1</td>
<td style="text-align:left"><a href="./1-1,%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B.md">1-1,结构化数据建模流程范例</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day2</td>
<td style="text-align:left"><a href="./1-2,%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B.md">1-2,图片数据建模流程范例</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day3</td>
<td style="text-align:left"><a href="./1-3,%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B.md">1-3,文本数据建模流程范例</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day4</td>
<td style="text-align:left"><a href="./1-4,%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B.md">1-4,时间序列数据建模流程范例</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E4%BA%8C%E3%80%81TensorFlow%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.md"><strong>二、TensorFlow的核心概念</strong></a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day5</td>
<td style="text-align:left"><a href="./2-1,%E5%BC%A0%E9%87%8F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.md">2-1,张量数据结构</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day6</td>
<td style="text-align:left"><a href="./2-2,%E4%B8%89%E7%A7%8D%E8%AE%A1%E7%AE%97%E5%9B%BE.md">2-2,三种计算图</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day7</td>
<td style="text-align:left"><a href="./2-3,%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E6%9C%BA%E5%88%B6.md">2-3,自动微分机制</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E4%B8%89%E3%80%81TensorFlow%E7%9A%84%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84.md"><strong>三、TensorFlow的层次结构</strong></a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day8</td>
<td style="text-align:left"><a href="./3-1,%E4%BD%8E%E9%98%B6API%E7%A4%BA%E8%8C%83.md">3-1,低阶API示范</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day9</td>
<td style="text-align:left"><a href="./3-2,%E4%B8%AD%E9%98%B6API%E7%A4%BA%E8%8C%83.md">3-2,中阶API示范</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day10</td>
<td style="text-align:left"><a href="./3-3,%E9%AB%98%E9%98%B6API%E7%A4%BA%E8%8C%83.md">3-3,高阶API示范</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E5%9B%9B%E3%80%81TensorFlow%E7%9A%84%E4%BD%8E%E9%98%B6API.md"><strong>四、TensorFlow的低阶API</strong></a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day11</td>
<td style="text-align:left"><a href="./4-1,%E5%BC%A0%E9%87%8F%E7%9A%84%E7%BB%93%E6%9E%84%E6%93%8D%E4%BD%9C.md">4-1,张量的结构操作</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day12</td>
<td style="text-align:left"><a href="./4-2,%E5%BC%A0%E9%87%8F%E7%9A%84%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97.md">4-2,张量的数学运算</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day13</td>
<td style="text-align:left"><a href="./4-3,AutoGraph%E7%9A%84%E4%BD%BF%E7%94%A8%E8%A7%84%E8%8C%83.md">4-3,AutoGraph的使用规范</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">0.5hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day14</td>
<td style="text-align:left"><a href="./4-4,AutoGraph%E7%9A%84%E6%9C%BA%E5%88%B6%E5%8E%9F%E7%90%86.md">4-4,AutoGraph的机制原理</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day15</td>
<td style="text-align:left"><a href="./4-5,AutoGraph%E5%92%8Ctf.Module.md">4-5,AutoGraph和tf.Module</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E4%BA%94%E3%80%81TensorFlow%E7%9A%84%E4%B8%AD%E9%98%B6API.md"><strong>五、TensorFlow的中阶API</strong></a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day16</td>
<td style="text-align:left"><a href="./5-1,%E6%95%B0%E6%8D%AE%E7%AE%A1%E9%81%93Dataset.md">5-1,数据管道Dataset</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day17</td>
<td style="text-align:left"><a href="./5-2,%E7%89%B9%E5%BE%81%E5%88%97feature_column.md">5-2,特征列feature_column</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day18</td>
<td style="text-align:left"><a href="./5-3,%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0activation.md">5-3,激活函数activation</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">0.5hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day19</td>
<td style="text-align:left"><a href="./5-4,%E6%A8%A1%E5%9E%8B%E5%B1%82layers.md">5-4,模型层layers</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day20</td>
<td style="text-align:left"><a href="./5-5,%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0losses.md">5-5,损失函数losses</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day21</td>
<td style="text-align:left"><a href="./5-6,%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87metrics.md">5-6,评估指标metrics</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day22</td>
<td style="text-align:left"><a href="./5-7,%E4%BC%98%E5%8C%96%E5%99%A8optimizers.md">5-7,优化器optimizers</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">0.5hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day23</td>
<td style="text-align:left"><a href="./5-8,%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0callbacks.md">5-8,回调函数callbacks</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E5%85%AD%E3%80%81TensorFlow%E7%9A%84%E9%AB%98%E9%98%B6API.md"><strong>六、TensorFlow的高阶API</strong></a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day24</td>
<td style="text-align:left"><a href="./6-1,%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B%E7%9A%843%E7%A7%8D%E6%96%B9%E6%B3%95.md">6-1,构建模型的3种方法</a></td>
<td style="text-align:right">⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day25</td>
<td style="text-align:left"><a href="./6-2,%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%843%E7%A7%8D%E6%96%B9%E6%B3%95.md">6-2,训练模型的3种方法</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day26</td>
<td style="text-align:left"><a href="./6-3,%E4%BD%BF%E7%94%A8%E5%8D%95GPU%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.md">6-3,使用单GPU训练模型</a></td>
<td style="text-align:right">⭐️⭐️</td>
<td style="text-align:right">0.5hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day27</td>
<td style="text-align:left"><a href="./6-4,%E4%BD%BF%E7%94%A8%E5%A4%9AGPU%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.md">6-4,使用多GPU训练模型</a></td>
<td style="text-align:right">⭐️⭐️</td>
<td style="text-align:right">0.5hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day28</td>
<td style="text-align:left"><a href="./6-5,%E4%BD%BF%E7%94%A8TPU%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.md">6-5,使用TPU训练模型</a></td>
<td style="text-align:right">⭐️⭐️</td>
<td style="text-align:right">0.5hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day29</td>
<td style="text-align:left"><a href="./6-6,%E4%BD%BF%E7%94%A8tensorflow-serving%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%9E%8B.md">6-6,使用tensorflow-serving部署模型</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">1hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right">day30</td>
<td style="text-align:left"><a href="./6-7,%E4%BD%BF%E7%94%A8spark-scala%E8%B0%83%E7%94%A8tensorflow%E6%A8%A1%E5%9E%8B.md">6-7,使用spark-scala调用tensorflow模型</a></td>
<td style="text-align:right">⭐️⭐️⭐️⭐️⭐️</td>
<td style="text-align:right">2hour</td>
<td style="text-align:right">✅</td>
</tr>
<tr>
<td style="text-align:right"> </td>
<td style="text-align:left"><a href="./%E5%90%8E%E8%AE%B0%EF%BC%9A%E4%B8%80%E4%B8%AA%E5%90%83%E8%B4%A7%E5%92%8C%E4%B8%80%E9%81%93%E8%8F%9C%E7%9A%84%E6%95%85%E4%BA%8B.md">后记：一个吃货和一道菜的故事</a></td>
<td style="text-align:right">⭐️</td>
<td style="text-align:right">0hour</td>
<td style="text-align:right">✅</td>
</tr>
</tbody>
</table>
<h2 id="step4-通过cnn-architectures项目复现常见cnn模型并阅读有关论文">Step4: 通过《CNN-Architectures》项目复现常见CNN模型，并阅读有关论文</h2>
<p><strong>CNN-Architectures：</strong>  https://github.com/Machine-Learning-Tokyo/CNN-Architectures/tree/master/Implementations</p>
<p>使用<code>tf.keras</code>API复现了一些常见CNN模型，包括：AlexNet、VGG、GoogLeNet、MobileNet、ResNet、Xception、SqueezeNet、DenseNet、ShuffleNet</p>
<h2 id="step5-通过deep-models-for-nlp-beginners项目学习nlp基础知识">Step5: 通过《Deep Models for NLP beginners》项目学习NLP基础知识</h2>
<p>Deep Models for NLP beginners：https://github.com/BrambleXu/nlp-beginner-guide-keras</p>
<p>包括词向量、情感分类以及实体识别</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[知识点备忘]]></title>
        <id>https://sunyanhust.github.io/post/zhi-shi-dian-bei-wang-he-todo/</id>
        <link href="https://sunyanhust.github.io/post/zhi-shi-dian-bei-wang-he-todo/">
        </link>
        <updated>2020-04-16T03:02:25.000Z</updated>
        <content type="html"><![CDATA[<h2 id="比较琐碎的知识">比较琐碎的知识</h2>
<ul>
<li>联想小新是开机按F2进入bios</li>
<li>windows上编辑的sh文件在linux上需要转换，转换软件为<code>doc2unix</code>，命令为<code>doc2unix filename</code></li>
<li>GPU机器之间拷贝文件直接scp加上机器的ip地址就可以，因为各个机器在同一局域网内</li>
</ul>
<h2 id="文本分类kaggle-kernel">文本分类kaggle kernel</h2>
<h3 id="基于lstm的多标签文本分类">基于LSTM的多标签文本分类</h3>
<p>kaggle kernel 链接： https://www.kaggle.com/rftexas/gru-lstm-rnn-101</p>
<p><strong>主要亮点</strong>：</p>
<ol>
<li>使用了tf.keras进行构建，很多代码可以复用为baseline</li>
<li>读取和加载Glove词向量</li>
<li>AUC作为评价标准</li>
<li>数据集处理为tf_dataset输入keras模型</li>
<li>在训练集训练后，在验证集继续训练两个epochs（小技巧，可能很有用）</li>
</ol>
<pre><code class="language-python">import gc
import pickle
import re
import string
import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from tensorflow.keras import backend as K
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras import initializers, regularizers, constraints
from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping
from tensorflow.keras.layers import Layer, Dense, Input, Embedding, SpatialDropout1D, Bidirectional, LSTM, \
    GlobalMaxPooling1D, GlobalAveragePooling1D
from tensorflow.keras.layers import concatenate
from tensorflow.keras.models import Model
from tqdm.notebook import tqdm

tqdm.pandas()

warnings.simplefilter('ignore')

# HYPERPARAMETERS
MAX_LEN = 220
MAX_FEATURES = 100000
EMBED_SIZE = 600
BATCH_SIZE = 128
N_EPOCHS = 5
LEARNING_RATE = 8e-4

# We will concatenate Crawl and GloVe embeddings
CRAWL_EMB_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'
GLOVE_EMB_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'


def display_training_curves(training, validation, title, subplot):
    &quot;&quot;&quot;
    Quickly display training curves
    &quot;&quot;&quot;
    if subplot % 10 == 1:
        plt.subplots(figsize=(10, 10), facecolor='#F0F0F0')
        plt.tight_layout()

    ax = plt.subplot(subplot)
    ax.set_facecolor('#F8F8F8')
    ax.plot(training)
    ax.plot(validation)
    ax.set_title('model' + title)
    ax.set_ylabel(title)
    ax.set_xlabel('epoch')
    ax.legend(['train', 'valid'])


def get_coeffs(word, *arr):
    return word, np.asarray(arr, dtype='float32')


def load_embeddings(embed_dir):
    with open(embed_dir, 'rb') as  infile:
        embeddings = pickle.load(infile)
        return embeddings


def build_embedding_matrix(word_index, embeddings_index, max_features, lower=True, verbose=True):
    embedding_matrix = np.zeros((max_features, 300))
    for word, i in tqdm(word_index.items(), len=(word_index.items())):
        if lower:
            word = word.lower()
        if i &gt;= max_features: continue
        try:
            embedding_vector = embeddings_index[word]
        except:
            embedding_vector = embeddings_index[&quot;unknown&quot;]
        if embedding_vector is not None:
            # words not found in embedding index will be all-zeros.
            embedding_matrix[i] = embedding_vector
    return embedding_matrix


def build_matrix(word_index, embeddings_index):
    embedding_matrix = np.zeros((len(word_index) + 1, 300))
    for word, i in word_index.items():
        try:
            embedding_matrix[i] = embeddings_index[word]
        except:
            embedding_matrix[i] = embeddings_index[&quot;unknown&quot;]
    return embedding_matrix


class Attention(Layer):
    &quot;&quot;&quot;
    Custom Keras attention layer
    Reference: https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043
    &quot;&quot;&quot;

    def __init__(self, step_dim, W_regularizer=None, b_regularizer=None,
                 W_constraint=None, b_constraint=None, bias=True, **kwargs):

        self.supports_masking = True

        self.bias = bias
        self.step_dim = step_dim
        self.features_dim = None
        super(Attention, self).__init__(**kwargs)

        self.param_W = {
            'initializer': initializers.get('glorot_uniform'),
            'name': '{}_W'.format(self.name),
            'regularizer': regularizers.get(W_regularizer),
            'constraint': constraints.get(W_constraint)
        }
        self.W = None

        self.param_b = {
            'initializer': 'zero',
            'name': '{}_b'.format(self.name),
            'regularizer': regularizers.get(b_regularizer),
            'constraint': constraints.get(b_constraint)
        }
        self.b = None

    def build(self, input_shape):
        assert len(input_shape) == 3

        self.features_dim = input_shape[-1]
        self.W = self.add_weight(shape=(input_shape[-1],),
                                 **self.param_W)

        if self.bias:
            self.b = self.add_weight(shape=(input_shape[1],),
                                     **self.param_b)

        self.built = True

    def compute_mask(self, input, input_mask=None):
        return None

    def call(self, x, mask=None):
        step_dim = self.step_dim
        features_dim = self.features_dim

        eij = K.reshape(
            K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))),
            (-1, step_dim))

        if self.bias:
            eij += self.b
        eij = K.tanh(eij)
        a = K.exp(eij)

        if mask is not None:
            a *= K.cast(mask, K.floatx())

        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())
        a = K.expand_dims(a)
        weighted_input = x * a
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0], self.features_dim


# We create a balanced

print('Loading train sets...')
train1 = pd.read_csv(&quot;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv&quot;)
train2 = pd.read_csv(&quot;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv&quot;)

train = pd.concat([
    train1[['comment_text', 'toxic']],
    train2[['comment_text', 'toxic']].query('toxic==1'),
    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)
])

del train1, train2

print('Loading validation sets...')
valid = pd.read_csv('/kaggle/input/val-en-df/validation_en.csv')

print('Loading test sets...')
test = pd.read_csv('/kaggle/input/test-en-df/test_en.csv')
sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')

misspell_dict = {&quot;aren't&quot;: &quot;are not&quot;, &quot;can't&quot;: &quot;cannot&quot;, &quot;couldn't&quot;: &quot;could not&quot;,
                 &quot;didn't&quot;: &quot;did not&quot;, &quot;doesn't&quot;: &quot;does not&quot;, &quot;don't&quot;: &quot;do not&quot;,
                 &quot;hadn't&quot;: &quot;had not&quot;, &quot;hasn't&quot;: &quot;has not&quot;, &quot;haven't&quot;: &quot;have not&quot;,
                 &quot;he'd&quot;: &quot;he would&quot;, &quot;he'll&quot;: &quot;he will&quot;, &quot;he's&quot;: &quot;he is&quot;,
                 &quot;i'd&quot;: &quot;I had&quot;, &quot;i'll&quot;: &quot;I will&quot;, &quot;i'm&quot;: &quot;I am&quot;, &quot;isn't&quot;: &quot;is not&quot;,
                 &quot;it's&quot;: &quot;it is&quot;, &quot;it'll&quot;: &quot;it will&quot;, &quot;i've&quot;: &quot;I have&quot;, &quot;let's&quot;: &quot;let us&quot;,
                 &quot;mightn't&quot;: &quot;might not&quot;, &quot;mustn't&quot;: &quot;must not&quot;, &quot;shan't&quot;: &quot;shall not&quot;,
                 &quot;she'd&quot;: &quot;she would&quot;, &quot;she'll&quot;: &quot;she will&quot;, &quot;she's&quot;: &quot;she is&quot;,
                 &quot;shouldn't&quot;: &quot;should not&quot;, &quot;that's&quot;: &quot;that is&quot;, &quot;there's&quot;: &quot;there is&quot;,
                 &quot;they'd&quot;: &quot;they would&quot;, &quot;they'll&quot;: &quot;they will&quot;, &quot;they're&quot;: &quot;they are&quot;,
                 &quot;they've&quot;: &quot;they have&quot;, &quot;we'd&quot;: &quot;we would&quot;, &quot;we're&quot;: &quot;we are&quot;,
                 &quot;weren't&quot;: &quot;were not&quot;, &quot;we've&quot;: &quot;we have&quot;, &quot;what'll&quot;: &quot;what will&quot;,
                 &quot;what're&quot;: &quot;what are&quot;, &quot;what's&quot;: &quot;what is&quot;, &quot;what've&quot;: &quot;what have&quot;,
                 &quot;where's&quot;: &quot;where is&quot;, &quot;who'd&quot;: &quot;who would&quot;, &quot;who'll&quot;: &quot;who will&quot;,
                 &quot;who're&quot;: &quot;who are&quot;, &quot;who's&quot;: &quot;who is&quot;, &quot;who've&quot;: &quot;who have&quot;,
                 &quot;won't&quot;: &quot;will not&quot;, &quot;wouldn't&quot;: &quot;would not&quot;, &quot;you'd&quot;: &quot;you would&quot;,
                 &quot;you'll&quot;: &quot;you will&quot;, &quot;you're&quot;: &quot;you are&quot;, &quot;you've&quot;: &quot;you have&quot;,
                 &quot;'re&quot;: &quot; are&quot;, &quot;wasn't&quot;: &quot;was not&quot;, &quot;we'll&quot;: &quot; will&quot;, &quot;tryin'&quot;: &quot;trying&quot;}


def _get_misspell(misspell_dict):
    misspell_re = re.compile('(%s)' % '|'.join(misspell_dict.keys()))
    return misspell_dict, misspell_re


def replace_typical_misspell(text):
    misspellings, misspellings_re = _get_misspell(misspell_dict)

    def replace(match):
        return misspellings[match.group(0)]

    return misspellings_re.sub(replace, text)


puncts = [',', '.', '&quot;', ':', ')', '(', '-', '!', '?', '|', ';', &quot;'&quot;, '$', '&amp;', '/', '[', ']',
          '&gt;', '%', '=', '#', '*', '+', '\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',
          '®', '`', '&lt;', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',
          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',
          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',
          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',
          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',
          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']


def clean_text(x):
    x = str(x)
    for punct in puncts + list(string.punctuation):
        if punct in x:
            x = x.replace(punct, f' {punct} ')
    return x


def clean_numbers(x):
    return re.sub(r'\d+', ' ', x)


def preprocess(train, valid, test, tfms):
    for tfm in tfms:
        print(tfm.__name__)
        train['comment_text'] = train['comment_text'].progress_apply(tfm)
        valid['comment_text_en'] = valid['comment_text_en'].progress_apply(tfm)
        test['content'] = test['content'].progress_apply(tfm)

    return train, valid, test


tfms = [replace_typical_misspell, clean_text, clean_numbers]
train, valid, test = preprocess(train, valid, test, tfms)

tokenizer = Tokenizer(num_words=MAX_FEATURES, filters='', lower=False)

print('Fitting tokenizer...')
tokenizer.fit_on_texts(list(train['comment_text']) + list(valid['comment_text_en']) + list(test['content_en']))
word_index = tokenizer.word_index

print('Building training set...')
X_train = tokenizer.texts_to_sequences(list(train['comment_text']))
y_train = train['toxic'].values

print('Building validation set...')
X_valid = tokenizer.texts_to_sequences(list(valid['comment_text_en']))
y_valid = valid['toxic'].values

print('Building test set ...')
X_test = tokenizer.texts_to_sequences(list(test['content_en']))

print('Padding sequences...')
X_train = pad_sequences(X_train, maxlen=MAX_LEN)
X_valid = pad_sequences(X_valid, maxlen=MAX_LEN)
X_test = pad_sequences(X_test, maxlen=MAX_LEN)

y_train = train.toxic.values
y_valid = valid.toxic.values

del tokenizer

print('Loading Crawl embeddings...')
crawl_embeddings = load_embeddings(CRAWL_EMB_PATH)

print('Loading GloVe embeddings...')
glove_embeddings = load_embeddings(GLOVE_EMB_PATH)

print('Building matrices...')
embedding_matrix_1 = build_matrix(word_index, crawl_embeddings)
embedding_matrix_2 = build_matrix(word_index, glove_embeddings)

print('Concatenating embedding matrices...')
embedding_matrix = np.concatenate([embedding_matrix_1, embedding_matrix_2], axis=1)

del embedding_matrix_1, embedding_matrix_2
del crawl_embeddings, glove_embeddings

gc.collect()

train_dataset = (
    tf.data.Dataset
        .from_tensor_slices((X_train, y_train))
        .repeat()
        .shuffle(2048)
        .batch(BATCH_SIZE)
)

valid_dataset = (
    tf.data.Dataset
        .from_tensor_slices((X_valid, y_valid))
        .batch(BATCH_SIZE)
        .cache()
)

test_dataset = (
    tf.data.Dataset
        .from_tensor_slices(X_test)
        .batch(BATCH_SIZE)
)


def build_model(word_index, embedding_matrix, verbose=True):
    &quot;&quot;&quot;
    credits go to: https://www.kaggle.com/thousandvoices/simple-lstm/
    &quot;&quot;&quot;
    sequence_input = Input(shape=(MAX_LEN,), dtype=tf.int32)

    embedding_layer = Embedding(*embedding_matrix.shape,
                                weights=[embedding_matrix],
                                trainable=False)

    x = embedding_layer(sequence_input)
    x = SpatialDropout1D(0.3)(x)
    x = Bidirectional(LSTM(256, return_sequences=True))(x)
    x = Bidirectional(LSTM(128, return_sequences=True))(x)

    att = Attention(MAX_LEN)(x)
    avg_pool1 = GlobalAveragePooling1D()(x)
    max_pool1 = GlobalMaxPooling1D()(x)
    hidden = concatenate([att, avg_pool1, max_pool1])

    hidden = Dense(512, activation='relu')(hidden)
    hidden = Dense(128, activation='relu')(hidden)
    out = Dense(1, activation='sigmoid')(hidden)
    model = Model(sequence_input, out)

    return model

model = build_model(word_index, embedding_matrix)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])
model.summary()

file_weights = 'best_model.h5'
# cb1 = ModelCheckpoint(file_weights, save_best_only=True)

cb2 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)
cb3 = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, cooldown=0, min_lr=0.0001)
cb4 = LearningRateScheduler(lambda epoch: LEARNING_RATE * (0.6 ** epoch))

n_steps = X_train.shape[0] // BATCH_SIZE

train_history = model.fit(
    train_dataset,
    steps_per_epoch=n_steps,
    validation_data=valid_dataset,
    callbacks=[cb4],
    epochs=N_EPOCHS
)

display_training_curves(
    train_history.history['loss'],
    train_history.history['val_loss'],
    'loss',
    211)

display_training_curves(
    train_history.history['auc'],
    train_history.history['val_auc'],
    'AUC',
    212)

n_steps = X_valid.shape[0] // BATCH_SIZE

train_history = model.fit(
    valid_dataset.repeat(),
    steps_per_epoch=n_steps,
    callbacks=[cb4],
    epochs=N_EPOCHS
)

preds = model.predict(test_dataset, verbose=1)
sub['toxic'] = preds

</code></pre>
<h3 id="基于bert的多标签文本分类使用tpu">基于BERT的多标签文本分类(使用TPU)</h3>
<p>kaggle kernel 链接： https://www.kaggle.com/sunyancn/jigsaw-tpu-bert-with-huggingface-and-keras</p>
<p><strong>主要亮点</strong>：</p>
<ol>
<li>使用了transformers的分词器进行快速分词</li>
<li>文本长度的可视化</li>
<li>TF Hub BERT模型的加载</li>
<li>TPU策略</li>
</ol>
<pre><code class="language-python"># %% [markdown]
# ## About this notebook
# 
# *[Jigsaw Multilingual Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification)* is the 3rd annual competition organized by the Jigsaw team. It follows *[Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)*, the original 2018 competition, and *[Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)*, which required the competitors to consider biased ML predictions in their new models. This year, the goal is to use english only training data to run toxicity predictions on many different languages, which can be done using multilingual models, and speed up using TPUs.
# 
# Many awesome notebooks has already been made so far. Many of them used really cool technologies like [Pytorch XLA](https://www.kaggle.com/theoviel/bert-pytorch-huggingface-starter). This notebook instead aims at constructing a **fast, concise, reusable, and beginner-friendly model scaffold**. It will focus on the following points:
# * **Using Tensorflow and Keras**: Tensorflow is a powerful framework, and Keras makes the training process extremely easy to understand. This is especially good for beginners to learn how to use TPUs, and for experts to focus on the modelling aspect.
# * **Using Huggingface's `transformers` library**: [This library](https://huggingface.co/transformers/) is extremely popular, so using this let you easily integrate the end result into your ML pipelines, and can be easily reused for your other projects.
# * **Native TPU usage**: The TPU usage is abstracted using the native `strategy` that was created using Tensorflow's `tf.distribute.experimental.TPUStrategy`. This avoids getting too much into the lower-level aspect of TPU management.
# * **Use a subset of the data**: Instead of using the entire dataset, we will only use the 2018 subset of the data available, which makes this much faster, all while achieving a respectable accuracy.

# %% [code]
import os
import warnings

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from kaggle_datasets import KaggleDatasets
import transformers
import traitlets
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm
from tokenizers import BertWordPieceTokenizer
from sklearn.metrics import roc_auc_score

warnings.simplefilter(&quot;ignore&quot;)

# %% [markdown]
# ## Helper Functions

# %% [code]
def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):
    tokenizer.enable_truncation(max_length=maxlen)
    tokenizer.enable_padding(max_length=maxlen)
    all_ids = []
    
    for i in tqdm(range(0, len(texts), chunk_size)):
        text_chunk = texts[i:i+chunk_size].tolist()
        encs = tokenizer.encode_batch(text_chunk)
        all_ids.extend([enc.ids for enc in encs])
    
    return np.array(all_ids)

# %% [code]
def build_model(transformer, loss='binary_crossentropy', max_len=512):
    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;)
    sequence_output = transformer(input_word_ids)[0]
    cls_token = sequence_output[:, 0, :]
    x = tf.keras.layers.Dropout(0.35)(cls_token)
    out = Dense(1, activation='sigmoid')(x)
    
    model = Model(inputs=input_word_ids, outputs=out)
    model.compile(Adam(lr=3e-5), loss=loss, metrics=[tf.keras.metrics.AUC()])
    
    return model

# %% [markdown]
# Cosine similarity calculates similarity by measuring the cosine of angle between two vectors. This is calculated as:
# ![](https://miro.medium.com/max/426/1*hub04IikybZIBkSEcEOtGA.png)
# 
# Cosine Similarity calculation for two vectors A and B [source]
# With cosine similarity, we need to convert sentences into vectors. One way to do that is to use bag of words with either TF (term frequency) or TF-IDF (term frequency- inverse document frequency). The choice of TF or TF-IDF depends on application and is immaterial to how cosine similarity is actually performed — which just needs vectors. TF is good for text similarity in general, but TF-IDF is good for search query relevance.

# %% [code]
# https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents
import nltk, string
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('punkt') # if necessary...


stemmer = nltk.stem.porter.PorterStemmer()
remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)

def stem_tokens(tokens):
    return [stemmer.stem(item) for item in tokens]

'''remove punctuation, lowercase, stem'''
def normalize(text):
    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))

vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')

def cosine_sim(text1, text2):
    tfidf = vectorizer.fit_transform([text1, text2])
    return ((tfidf * tfidf.T).A)[0,1]

# %% [markdown]
# ## TPU Configs

# %% [code]
AUTO = tf.data.experimental.AUTOTUNE

# Create strategy from tpu
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
strategy = tf.distribute.experimental.TPUStrategy(tpu)

# Data access
#GCS_DS_PATH = KaggleDatasets().get_gcs_path('kaggle/input/') 

# %% [markdown]
# ## Create fast tokenizer

# %% [code]
# First load the real tokenizer
tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')

# Save the loaded tokenizer locally
save_path = '/kaggle/working/distilbert_base_uncased/'
if not os.path.exists(save_path):
    os.makedirs(save_path)
tokenizer.save_pretrained(save_path)

# Reload it with the huggingface tokenizers library
fast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased/vocab.txt', lowercase=True)
fast_tokenizer

# %% [markdown]
# ## Load text data into memory

# %% [code]
train1 = pd.read_csv(&quot;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv&quot;)
train2 = pd.read_csv(&quot;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv&quot;)

valid = pd.read_csv('/kaggle/input/val-en-df/validation_en.csv')
test1 = pd.read_csv('/kaggle/input/test-en-df/test_en.csv')
test2 = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_test_translated.csv')
sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')

# %% [code]
test2.head()

# %% [markdown]
# ## Test dataset comparision

# %% [code]
plt.figure(figsize=(12, 8))

sns.distplot(train1.comment_text.str.len(), label='train')
sns.distplot(test1.content_en.str.len(), label='test1')
sns.distplot(test2.translated.str.len(), label='test2')
plt.legend();

# %% [code]
plt.figure(figsize=(12, 8))

sns.distplot(train1.comment_text.str.len(), label='train')
sns.distplot(test1.content_en.str.len(), label='test1')
sns.distplot(test2.translated.str.len(), label='test2')
plt.xlim([0, 512])
plt.legend();

# %% [markdown]
# Lets calculate cosine similarity two translated test datasets.

# %% [code]
test_set_similarity = [cosine_sim(t1, t2) for t1, t2 in tqdm(zip(test1.content_en, test2.translated))]

plt.figure(figsize=(12, 8))

sns.distplot(test_set_similarity);

# %% [markdown]
# ## Fast encode

# %% [code]
x_train = fast_encode(train1.comment_text.astype(str), fast_tokenizer, maxlen=512)
x_valid = fast_encode(valid.comment_text_en.astype(str), fast_tokenizer, maxlen=512)
x_test1 = fast_encode(test1.content_en.astype(str), fast_tokenizer, maxlen=512)
x_test2 = fast_encode(test2.translated.astype(str), fast_tokenizer, maxlen=512)

y_train = train1.toxic.values
y_valid = valid.toxic.values

# %% [markdown]
# ## Build datasets objects

# %% [code]
train_dataset = (
    tf.data.Dataset
    .from_tensor_slices((x_train, y_train))
    .repeat()
    .shuffle(2048)
    .batch(64)
    .prefetch(AUTO)
)

valid_dataset = (
    tf.data.Dataset
    .from_tensor_slices((x_valid, y_valid))
    .batch(64)
    .cache()
    .prefetch(AUTO)
)

test_dataset = [(
    tf.data.Dataset
    .from_tensor_slices(x_test1)
    .batch(64)
),
    (
    tf.data.Dataset
    .from_tensor_slices(x_test2)
    .batch(64)
)]

# %% [markdown]
# # Focal Loss

# %% [code]
from tensorflow.keras import backend as K

def focal_loss(gamma=2., alpha=.2):
    def focal_loss_fixed(y_true, y_pred):
        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))
        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))
        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))
    return focal_loss_fixed

# %% [markdown]
# ## Load model into the TPU

# %% [code]
%%time
with strategy.scope():
    transformer_layer = transformers.TFBertModel.from_pretrained('bert-base-uncased')
    model = build_model(transformer_layer, loss=focal_loss(gamma=1.5), max_len=512)
model.summary()

# %% [markdown]
# ## RocAuc Callback

# %% [code]
from tensorflow.keras.callbacks import Callback 

class RocAucCallback(Callback):
    def __init__(self, test_data, score_thr):
        self.test_data = test_data
        self.score_thr = score_thr
        self.test_pred = []
        
    def on_epoch_end(self, epoch, logs=None):
        if logs['val_auc'] &gt; self.score_thr:
            print('\nRun TTA...')
            for td in self.test_data:
                self.test_pred.append(self.model.predict(td))

# %% [markdown]
# # LrScheduler

# %% [code]
def build_lrfn(lr_start=0.000001, lr_max=0.000002, 
               lr_min=0.0000001, lr_rampup_epochs=7, 
               lr_sustain_epochs=0, lr_exp_decay=.87):
    lr_max = lr_max * strategy.num_replicas_in_sync

    def lrfn(epoch):
        if epoch &lt; lr_rampup_epochs:
            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start
        elif epoch &lt; lr_rampup_epochs + lr_sustain_epochs:
            lr = lr_max
        else:
            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min
        return lr
    
    return lrfn

# %% [code]
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 7))

_lrfn = build_lrfn()
plt.plot([i for i in range(35)], [_lrfn(i) for i in range(35)]);

# %% [markdown]
# ## Train Model

# %% [code]
roc_auc = RocAucCallback(test_dataset, 0.9195)
lrfn = build_lrfn()
lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)

train_history = model.fit(
    train_dataset,
    steps_per_epoch=150,
    validation_data=valid_dataset,
    callbacks=[lr_schedule, roc_auc],
    epochs=35
)

# %% [markdown]
# ## Submission

# %% [code]
sub['toxic'] = np.mean(roc_auc.test_pred, axis=0)
sub.to_csv('submission.csv', index=False)

# %% [markdown]
# # Reference
# * [Jigsaw TPU: DistilBERT with Huggingface and Keras](https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras)
# * [inference of bert tpu model ml w/ validation](https://www.kaggle.com/abhishek/inference-of-bert-tpu-model-ml-w-validation)
# * [Overview of Text Similarity Metrics in Python](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50)
# * [test-en-df](https://www.kaggle.com/bamps53/test-en-df)
# * [val_en_df](https://www.kaggle.com/bamps53/val-en-df)
# * [Jigsaw multilingual toxic - test translated](https://www.kaggle.com/kashnitsky/jigsaw-multilingual-toxic-test-translated)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latex画神经网络图]]></title>
        <id>https://sunyanhust.github.io/post/latex-hua-shen-jing-wang-luo-tu/</id>
        <link href="https://sunyanhust.github.io/post/latex-hua-shen-jing-wang-luo-tu/">
        </link>
        <updated>2020-04-12T03:56:10.000Z</updated>
        <content type="html"><![CDATA[<h2 id="bilstm">BiLSTM</h2>
<h3 id="效果图">🥙效果图</h3>
<figure data-type="image" tabindex="1"><img src="https://sunyanhust.github.io/post-images/1587243946926.png" alt="" loading="lazy"></figure>
<h3 id="代码">📜代码</h3>
<pre><code class="language-tex">\documentclass[crop, tikz]{standalone}
\usepackage{tikz}

\usetikzlibrary{positioning}

\begin{document}
\begin{tikzpicture}
	\node[rectangle] (Y0) at (0, 0) {$\dots$};
	\node[rectangle, draw, right=2em of Y0, minimum height=1cm, minimum width=1cm] (RNN) {LSTM$_\rightarrow$};
	\node[rectangle, right=of RNN, draw, minimum height=1cm, minimum width=1cm] (RNN2) {LSTM$_\rightarrow$};
	\node[rectangle, right=of RNN2, draw, minimum height=1cm, minimum width=1cm] (RNN3) {LSTM$_\rightarrow$};
			
	\node[rectangle, right= of RNN3, draw, minimum height=1cm, minimum width=1cm] (RNN4) {LSTM$_\rightarrow$};
	\node[rectangle, right=2em of RNN4] (RNN5) {$\dots$};
			
			
	\node[rectangle, above=of RNN4, draw, minimum height=1cm, minimum width=1cm] (R25) {LSTM$_\leftarrow$};
	\node[rectangle, left=of R25, minimum height=1cm, minimum width=1cm, draw] (R24) {LSTM$_\leftarrow$};
	\node[rectangle, left=of R24, draw, minimum height=1cm, minimum width=1cm] (R23) {LSTM$_\leftarrow$};
	\node[rectangle, left=of R23, draw, minimum height=1cm, minimum width=1cm] (R22) {LSTM$_\leftarrow$};
	\node[rectangle, left=2em of R22] (R21) {$\dots$};
	\node[right=2em of R25] (Y20) {$\dots$};
			
	\node[below=of RNN] (X1) {$\vec{x}_1$};
	\node[below=of RNN2] (X2) {$\vec{x}_2$};
	\node[below=of RNN3] (X3) {$\vec{x}_3$};
	\node[below=of RNN4] (X4) {$\vec{x}_4$};
	\node[above=of R25] (Y5) {$\vec{h}_4$};
	\node[above=of R24] (Y4) {$\vec{h}_3$};
	\node[above=of R23] (Y3) {$\vec{h}_2$};
	\node[above=of R22] (Y2) {$\vec{h}_1$};
			
	\draw[-stealth, thick] (X1) -- (RNN);
	\draw[-stealth, thick] (X2) -- (RNN2);
	\draw[-stealth, thick] (X3) -- (RNN3);
	\draw[-stealth, thick] (X4) -- (RNN4);
	\draw[-stealth, thick, densely dotted] (Y0) -- (RNN);
	\draw[-stealth, thick] (RNN) -- node[above, pos=0.35] {$\vec{h}_2^\rightarrow$} (RNN2);
	\draw[-stealth, thick] (RNN2) -- node[above, pos=0.35] {$\vec{h}_3^\rightarrow$} (RNN3);
	\draw[-stealth, thick] (RNN3) -- node[above, pos=0.35] {$\vec{h}_4^\rightarrow$} (RNN4);
	\draw[-stealth, densely dotted, thick] (RNN4) -- (RNN5);
	\node[below=4em of Y0] (d) {\dots};
	\node[below=4em of RNN5] (d) {\dots};
			
	\path[-stealth, ultra thick, white] (X1) edge[bend left=45] (R22);
	\path[-stealth, thick] (X1) edge[bend left=45] (R22);
	\path[-stealth, ultra thick, white] (X2) edge[bend left=45] (R23);
	\path[-stealth, thick] (X2) edge[bend left=45] (R23);
	\path[-stealth, ultra thick, white] (X3) edge[bend left=45] (R24);
	\path[-stealth, thick] (X3) edge[bend left=45] (R24);
	\path[-stealth, ultra thick, white] (X4) edge[bend left=45] (R25);
	\path[-stealth, thick] (X4) edge[bend left=45] (R25);
	\draw[-stealth, densely dotted, thick] (Y20) -- (R25);
			
	\draw[-stealth, thick] (R22) -- (Y2);
	\draw[-stealth, thick] (R23) -- (Y3);
	\draw[-stealth, thick] (R24) -- (Y4);
	\draw[-stealth, thick] (R25) -- (Y5);
		
	\draw[stealth-, densely dotted, thick] (R21) -- (R22);
	\draw[stealth-, thick] (R22) -- node[above, pos=0.65] {$\vec{h}_3^\leftarrow$} (R23);
	\draw[stealth-, thick] (R23) -- node[above, pos=0.65] {$\vec{h}_4^\leftarrow$} (R24);
	\draw[stealth-, thick] (R24) -- node[above, pos=0.65] {$\vec{h}_5^\leftarrow$} (R25);
	\draw[-stealth, densely dotted, thick] (Y20) -- (R25);	
			
	\path[-stealth, ultra thick, white] (RNN) edge[bend right=45] (Y2);
	\path[-stealth, thick] (RNN) edge[bend right=45] (Y2);
	\path[-stealth, ultra thick, white] (RNN2) edge[bend right=45] (Y3);
	\path[-stealth, thick] (RNN2) edge[bend right=45] (Y3);
	\path[-stealth, ultra thick, white] (RNN3) edge[bend right=45] (Y4);
	\path[-stealth, thick] (RNN3) edge[bend right=45] (Y4);
	\path[-stealth, ultra thick, white] (RNN4) edge[bend right=45] (Y5);
	\path[-stealth, thick] (RNN4) edge[bend right=45] (Y5);
			
\end{tikzpicture}
\end{document}
</code></pre>
<h2 id="lstm单元">LSTM单元</h2>
<h3 id="效果图-2">🥙效果图</h3>
<figure data-type="image" tabindex="2"><img src="https://sunyanhust.github.io/post-images/1587241071902.PNG" alt="" loading="lazy"></figure>
<h3 id="代码-2">📜代码</h3>
<pre><code class="language-tex">\documentclass[crop, tikz]{standalone}
\usepackage{tikz}

\usepackage{bm}
\usepackage{relsize}
\usepackage{pgfplots}
 
\usetikzlibrary{arrows,shapes, decorations.pathmorphing,backgrounds,positioning}

\begin{document}
\begin{tikzpicture}

	\node[rectangle, rounded corners=10, minimum width=20em, minimum height=12em, draw, very thick, fill=white] (lstm) at (0, 0) {};
	
	\node[rectangle, draw] at (-2.5, -0.8) (s1){$\sigma$};
	\node[rectangle, draw, right=1em of s1] (s2) {$\sigma$};
	\node[rectangle, draw, right=1em of s2] (t1) {$tanh$};
	\node[rectangle, draw, right=1em of t1] (s3) {$\sigma$};
	\node[circle, draw, above=2em of t1, inner sep=0em] (m1) {$\otimes$};
	\node[circle, draw, above=6em of s1, inner sep=0em] (m2) {$\otimes$};
	\node[circle, draw, right=6.55em of m2, inner sep=0em] (p1) {$\oplus$};
	\node[circle, draw, right=4.5em of m1, inner sep=0em] (m3) {$\otimes$};
	\node[rectangle, draw, above=1em of m3, inner sep=0.2em] (tt) {$tanh$ };
	
	\node[circle, draw, below=1em of s1, inner sep=0em] (conc) {$||$};
	
	\node[below=5em of s1] (xt) {$\vec{x}_t$};
	\node[left=3em of conc] (ht1) {$\vec{h}_{t-1}$};
	\node[left=3em of m2] (ct1) {$c_{t-1}$};
	\node[right=18em of m2] (ct) {$c_t$};
	\node[right=18em of conc] (ht) {$\vec{h}_t$};
	\node[] (yt) at (3, 3) {$\vec{h}_t$};
	
	\draw[-stealth, line width=1mm, white] (xt) -- (conc);
	\draw[-stealth, very thick] (xt) -- (conc);
	\draw[-stealth, line width=1mm, white] (ht1) -- (conc);
	\draw[-stealth, very thick] (ht1) -- (conc);
	
	\draw[-stealth, very thick] (conc) -- (s1);
	\path[-stealth, very thick] (conc) edge[bend right] (s2.south);
	\path[-stealth, very thick] (conc) edge[bend right] (t1.south);
	\path[-stealth, very thick] (conc) edge[bend right] (s3.south);
	\draw[-stealth, very thick] (s1) -- node[left] {$f_t$} (m2);
	\draw[-stealth, very thick] (s2) edge[bend left] node[above] {$i_t$} (m1.west);
	\draw[-stealth, very thick] (t1) -- node[right] {$\tilde{c_t}$} (m1);
	\draw[-stealth, very thick] (m1) -- (p1);
	\draw[-stealth, line width=1mm, white] (ct1) -- (m2);
	\draw[-stealth, very thick] (ct1) -- (m2);
	\draw[-stealth, very thick] (m2) -- (p1);
	\draw[-stealth, very thick] (s3) edge[bend left] node[left] {$o_t$} (m3.west);
	
	\draw[-stealth, line width=1mm, white] (p1) -- (ct);
	\draw[-stealth, very thick] (p1) -- (ct);
	\draw[-stealth, very thick] (tt) -- (m3);
	\draw[-stealth, line width=1mm, white] (m3) edge[bend right] (ht.west);
	\draw[-stealth, very thick] (m3) edge[bend right] (ht.west);
	
	\draw[-stealth ,very thick] (p1) edge[bend right] (tt.west);
	\draw[-stealth, line width=1mm,white] (m3) edge[bend right] (yt.south);
	\draw[-stealth, very thick] (m3) edge[bend right] (yt.south);
			
\end{tikzpicture}
\end{document}
</code></pre>
<h2 id="自注意力">自注意力</h2>
<h3 id="效果图-3">🥙效果图</h3>
<figure data-type="image" tabindex="3"><img src="https://sunyanhust.github.io/post-images/1587241576659.png" alt="" loading="lazy"></figure>
<h3 id="代码-3">📜代码</h3>
<pre><code class="language-tex">\documentclass[crop, tikz]{standalone}
\usepackage{tikz}

\usetikzlibrary{positioning}

\begin{document}
\begin{tikzpicture}

	\node (X1) {$\vec{h}_{1}$};

	\node[rectangle, right= 0.5em of X1] (x_dots_1) {$\dots$};

	\node[right=0.5em of x_dots_1] (Xj) {$\vec{h}_{j}$};

	\node[rectangle, right= 1em of Xj] (x_dots_2) {$\dots$};

	\node[right=1em of x_dots_2] (Xn) {$\vec{h}_{n}$};

	\node[rectangle, draw, ultra thick, above=of X1] (attn1) {\large $a_\phi$};

	\node[rectangle, draw, ultra thick, above=of Xj] (attnj) {\large $a_\phi$};

	\node[rectangle, draw, ultra thick, above=of Xn] (attnn) {\large $a_\phi$};


	\draw[-stealth, thick] (X1) -- (attn1);
	\draw[-stealth, thick] (Xj) -- (attn1);

	\draw[-stealth, thick] (Xj) -- (attnj);
	\draw[-stealth, thick] ([xshift=3em]Xj) -- (attnj);
	
	\draw[-stealth, thick] (Xj) -- (attnn);
	\draw[-stealth, thick] (Xn) -- (attnn);
	
	\node[above= of attn1, opacity=0.2] (alpha1j) {$\alpha_{1,j}$};
	\node[above= of attnj, opacity=1] (alphajj) {$\alpha_{j,j}$};
	\node[above= of attnn, opacity=0.6] (alphanj) {$\alpha_{n,j}$};
	
	\node[circle, draw, above=of alpha1j] (times1) {$\times$};
	\node[circle, draw, above=of alphajj] (timesj) {$\times$};
	\node[circle, draw, above=of alphanj] (timesn) {$\times$};
	
	\node[rectangle, draw, above=of timesj] (sum) {$\Sigma$};

	\node[above=1em of sum] (x_tprim) {$\vec{h}_j'$};

	\draw[-stealth, line width=1.5mm, white] (attn1) -- (alpha1j);
	\draw[-stealth, thick, opacity=0.2] (attn1) -- (alpha1j);
	\draw[-stealth, line width=1.5mm, white] (attnj) -- (alphajj);
	\draw[-stealth, thick, opacity=1] (attnj) -- (alphajj);
	\draw[-stealth, line width=1.5mm, white] (attnn) -- (alphanj);
	\draw[-stealth, thick, opacity=0.6] (attnn) -- (alphanj);
	
	\draw[-stealth, white, line width=1.5mm] (X1) edge[bend right=30] (times1);
	\draw[-stealth, thick] (X1) edge[bend right=30] node[rectangle, draw, fill=white, midway] {$f_\psi$} (times1);
	\draw[-stealth, white, line width=1.5mm] (Xj) edge[bend right=30] (timesj);
	\draw[-stealth, thick] (Xj) edge[bend right=30] node[rectangle, draw, fill=white, midway] {$f_\psi$} (timesj);
	\draw[-stealth, thick] (Xn) edge[bend right=30] node[rectangle, draw, fill=white, midway] {$f_\psi$} (timesn);

	\draw[-, line width=1.5mm, white] (times1) -- (sum);
	\draw[-stealth, thick] (times1) -- (sum);
	\draw[-, line width=1.5mm, white] (timesj) -- (sum);
	\draw[-stealth, thick] (timesj) -- (sum);
	\draw[-stealth, thick] (timesn) -- (sum);
	\draw[-stealth, thick] (times1) -- (sum);
		
	\draw[-stealth, line width=1.5mm, white] (alpha1j) -- (times1);
	\draw[-stealth, thick, opacity=0.2] (alpha1j) -- (times1);
	\draw[-stealth, line width=1.5mm, white] (alphajj) -- (timesj);
	\draw[-stealth, thick, opacity=1] (alphajj) -- (timesj);
	\draw[-stealth, line width=1.5mm, white] (alphanj) -- (timesn);
	\draw[-stealth, thick, opacity=0.6] (alphanj) -- (timesn);

	\draw[-stealth, thick] (sum) -- (x_tprim);

\end{tikzpicture}
\end{document}
</code></pre>
<h2 id="rnn">RNN</h2>
<h3 id="效果图-4">🥙效果图</h3>
<figure data-type="image" tabindex="4"><img src="https://sunyanhust.github.io/post-images/1587243725699.png" alt="" loading="lazy"></figure>
<h3 id="代码-4">📜代码</h3>
<pre><code class="language-tex">\documentclass[crop, tikz]{standalone}
\usepackage{tikz}

\usetikzlibrary{positioning}

\begin{document}
\begin{tikzpicture}
	\node[rectangle] (Y0) at (0, 0) {$\dots$};
	\node[rectangle, draw, right=2em of Y0, minimum height=1cm, minimum width=1cm] (RNN) {RNN};
	\node[rectangle, right=of RNN, draw, minimum height=1cm, minimum width=1cm] (RNN2) {RNN};
	\node[rectangle, right=of RNN2, draw, minimum height=1cm, minimum width=1cm] (RNN3) {RNN};
	\node[rectangle, right= of RNN3, draw, minimum height=1cm, minimum width=1cm] (RNN4) {RNN};
	\node[rectangle, right=2em of RNN4] (RNN5) {$\dots$};
			
	\node[below=of RNN] (X1) {$\vec{x}_1$};
	\node[below=of RNN2] (X2) {$\vec{x}_2$};
	\node[below=of RNN3] (X3) {$\vec{x}_3$};
	\node[below=of RNN4] (X4) {$\vec{x}_4$};
	\node[above=of RNN4] (Y5) {$\vec{h}_4$};
	\node[above=of RNN3] (Y4) {$\vec{h}_3$};
	\node[above=of RNN2] (Y3) {$\vec{h}_2$};
	\node[above=of RNN] (Y2) {$\vec{h}_1$};
			
	\draw[-stealth, thick] (X1) -- (RNN);
	\draw[-stealth, thick] (X2) -- (RNN2);
	\draw[-stealth, thick] (X3) -- (RNN3);
	\draw[-stealth, thick] (X4) -- (RNN4);
	
	\draw[-stealth, thick] (RNN) -- (Y2);
	\draw[-stealth, thick] (RNN2) -- (Y3);
	\draw[-stealth, thick] (RNN3) -- (Y4);
	\draw[-stealth, thick] (RNN4) -- (Y5);
	
    \draw[-stealth, densely dotted, thick] (Y0) -- (RNN);
    \draw[-stealth, densely dotted, thick] (RNN) -- (RNN2);
    \draw[-stealth, densely dotted, thick] (RNN2) -- (RNN3);
    \draw[-stealth, densely dotted, thick] (RNN3) -- (RNN4);
    \draw[-stealth, densely dotted, thick] (RNN4) -- (RNN5);
			
\end{tikzpicture}
\end{document}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PaddleHub使用示例]]></title>
        <id>https://sunyanhust.github.io/post/paddlehub/</id>
        <link href="https://sunyanhust.github.io/post/paddlehub/">
        </link>
        <updated>2020-04-12T03:04:18.000Z</updated>
        <content type="html"><![CDATA[<p>最近用了一下PaddleHub，感觉还挺好用的。这里两个使用PaddleHub的示例。</p>
<h2 id="分词">分词</h2>
<p>这个分词和官网的分词效果一样，觉得比jieba之类的要好。</p>
<pre><code class="language-python"># pip install pyahocorasick
# https://www.paddlepaddle.org.cn/hubdetail?name=lac&amp;en_category=LexicalAnalysis
import paddlehub as hub

temp_user_dict = [
    dict(word='自然', tag='n', freq='10000')
]


def make_dict(user_dicts):
    with open('user.dict', 'w') as f:
        for user_dict in user_dicts:
            f.write(user_dict['word'] + '\t' +
                    user_dict['tag'] + '\t' +
                    user_dict['freq'] + '\n')


make_dict(temp_user_dict)

lac = hub.Module(name='lac')
lac.set_user_dict(dict_path='user.dict')
results = lac.lexical_analysis(texts=['我爱自然语言处理'],
                               use_gpu=False,
                               batch_size=1,
                               return_tag=True)

for result in results:
    print(result[&quot;word&quot;])
    print(result[&quot;tag&quot;])
</code></pre>
<h2 id="阅读理解">阅读理解</h2>
<pre><code class="language-python">import paddlehub as hub

module = hub.Module(name=&quot;roberta_wwm_ext_chinese_L-24_H-1024_A-16&quot;)
inputs, outputs, program = module.context(trainable=True, max_seq_len=384)
dataset = hub.dataset.CMRC2018()

reader = hub.reader.ReadingComprehensionReader(
    dataset=dataset,
    vocab_path=module.get_vocab_path(),
    max_seq_len=384)

strategy = hub.AdamWeightDecayStrategy(
    learning_rate=5e-5,
    weight_decay=0.01,
    warmup_proportion=0.1
)

config = hub.RunConfig(use_cuda=False, num_epoch=2, batch_size=12, strategy=strategy)
seq_output = outputs[&quot;sequence_output&quot;]

# feed_list的Tensor顺序不可以调整
feed_list = [
    inputs[&quot;input_ids&quot;].name,
    inputs[&quot;position_ids&quot;].name,
    inputs[&quot;segment_ids&quot;].name,
    inputs[&quot;input_mask&quot;].name,
]

reading_comprehension_task = hub.ReadingComprehensionTask(
    data_reader=reader,
    feature=seq_output,
    feed_list=feed_list,
    config=config,
    sub_task=&quot;cmrc2018&quot;)

reading_comprehension_task.finetune_and_eval()

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[对待婚姻的态度]]></title>
        <id>https://sunyanhust.github.io/post/guan-yu-hun-yin/</id>
        <link href="https://sunyanhust.github.io/post/guan-yu-hun-yin/">
        </link>
        <updated>2020-04-06T13:42:02.000Z</updated>
        <content type="html"><![CDATA[<p>今天晚上，得知我最好的朋友离婚了。孩子才1岁，同样身为父亲的我感到难过，离婚对孩子来说影响太大。</p>
<p>我没资格说这些。不过，至少我的婚姻算凑合，我也有把握延续这种凑合。我写这篇文章，就是希望有更多的人能把握自己的婚姻。就算我说的都是废话，如果能引起朋友们在婚前对婚姻多一些思考，就达到目的了。</p>
<p>每个人对婚姻的态度都是不同的，随便找个人过日子的，找个女人帮自己生孩的，找个过夜不收钱的，这些所谓的“婚姻”就不在讨论范围了，这里和大家讨论的，是高质量的婚姻。</p>
<p>每个人的理解都不一样，在我看来，婚姻的状态有很多种，但是让人舒服的婚姻，一定都是有爱的。</p>
<p>有人说，爱情是有保质期的。我说，有保质期的不叫爱情。那叫激情，激情夹杂的东西太多，性欲，感动，内疚，憧憬，有太多太多的杂质，这样的情感确实难以持久。何况，激情往往是精心呵护起来的，一旦丢失了精心呵护的动力，褪色太快。</p>
<p>泡妞的时候，激情是最好的工具。然而面临结婚选择时，作为男人，一定要理清自己的头脑，祛除激情的成分。这个思考的过程非常重要，婚姻是没有回头路的。别以为大不了还可以离婚。离婚不是解脱，是又一个麻烦的开始。</p>
<p>如何确定自己爱不爱一个女人，这是非常关键的一步。我很肯定的说，很少男人清楚这个问题。男人的生理天性决定了对女人的选择很大因素是外貌。但是婚姻是反人类天性的。所以当男人选择了婚姻，就注定要克服自己的生理天性。</p>
<p>男人的生理天性，说白就是希望保质保量的遗传自己的基因，比如尽可能的和更多女人上床，尽可能的选择更漂亮，基因更优秀的女人上床。</p>
<p>而婚姻，恰恰相反，男人只能选择一个女人。而且要知道，眼前这个将要嫁给自己的美女，不几年就会生孩子，眼角会有皱纹，乳房会下垂，乳头会变的很黑，屁股会变形，小肚子会出来，还会有难看的斑纹。更可怕的是，男人的生理天性决定了男人老和同一个女人上床，是会腻的。所以，没有爱的婚姻是危险的。因为没有克服生理欲望的信念。</p>
<p>​       兄弟们永远别在婚前信誓旦旦的说自己不是那种包二奶的人。50年代出身那批人，道德观念比我们重的多，老婆还都是共过患难的糟糠，不一样大量的出轨？所以，在一夫一妻的制度下，保障婚姻的根本，还是得依靠爱情。其他约束在人性本能的拉扯下都是脆弱的。</p>
<p>据我观察，很多失败婚姻的元凶就是性。性对男人的诱惑是致命的，不少男人就是因为和女人上了床就顺理成章的结婚了。是好是坏全看运气。压根不思考有没有共同语言，遇到问题双方的沟通方式彼此能否接受，这些关键的问题不去想。就因为上了床就结婚。这是很多悲剧的来源。用人性本能来适应反人性本能的婚姻制度。这是非常愚蠢的。更愚蠢的是很多女人以性为工具来对待婚姻。说到这里才发现，婚姻真是个大话题。每个人去领结婚证的时候，都没想过有一天会去拿离婚证。可惜，离婚的人越来越多。据统计80后的离婚率快到百分四十了。</p>
<p>男人，只有把性欲，感动等等等等因素全抛开，才能真正认识自己对一个女人的感情是不是爱。爱不能解决所有问题，但是能给男人解决问题的动力。没有动力经营的婚姻，麻烦很多。因为婚姻的问题很多。</p>
<p>结婚对男人而言，纯粹是责任。有了承担责任的决心，才敢谈婚姻，所以，结婚一定要找一个自己爱的女人。自己余下的生命全部为之付出的家庭，女主人不是自己真正爱的，太悲剧了。因为婚姻唯一能回报给男人的，仅仅是一只能牵着自己走向死亡的手。千万别奢望婚姻回报给自己什么，任何奢望只会添加双方的压力。做好丈夫该做的，自然得到应得的。</p>
<p>很重要的一点，不是平时有没有共同语言，而是有分歧和问题的沟通。说白了，如果男人是个讲道理的人，女人也得是个懂道理的人，如果男人是个喜欢用拳头说话的人，女人也得是个肯挨拳头的人。女人如果喜欢唠叨，男人就得听得惯。总之，两个人必须得有个拿出统一意见的程序，且双方都能接受。最好是都乐意接受。其实这一条对于真正相爱的人来讲不是问题，真正相爱的两个人永远是站在对方立场思考问题的。</p>
<p>有情饮水饱，只能饱一顿。男人没结婚，是潇洒的，一结婚，负担就来了。两个人的结合所带的压力肯定是巨大的，必须要考虑双方的承受能力，自己没有能力负担，就不要害人害己了。女朋友如果喜欢钱，有钱就娶，没钱就不要砸锅卖铁娶回家了。</p>
<p>有些负担，背上就是一辈子的。对婚姻而言，肯陪自己一起奋斗的女人，才真正应该珍惜。单单是两个人一起改善生活的过程，就已经是婚姻宝贵的财富了。别说现在这样的女人少，不少。老盯着花枝招展想靠嫁人改变命运的年轻女人，就别怪女人现实。自己从未规划过未来，就别怪女人不肯陪自己一起奋斗。</p>
<p>我一直提倡婚前性行为，性生活得到满足的男人，才更容易发现女人其他的魅力。而现实的情况是，不少好女人还剩着，男人却在追逐性的过程中迷失了自己。因色而结合，女人色衰之后，没本事的男人继续鬼混，有本事的换个女人凑合。这样的风气还有个很烦的影响，女人越来越在意自己的形象，化妆时间越来越长，衣领越来越低，裙子越来越短。而充电的时间越来越少，独立意识越来越淡。</p>
<p>男人一边追逐女色，一边抱怨女人素质越来越低。</p>
<p>女人呢？一边抱怨男人肤浅，一边不断迎合肤浅的审美。</p>
<p>婚姻不是人生的全部，两个人携手一生，相助则利，相阻则损。性格互补也好，志同道合也好，彼此成为对方的助力，真的很重要。如果和一个女人婚前就感觉疲惫。相信我，别结婚了。家是港湾，是一个让男人快到家门就会不自觉加快脚步的地方。是一个一回家就会彻底放松，卸下所有伪装的地方。是一个所有笑容都发自真心的地方。</p>
<p>怎样区分激情和爱情？其实很好区分的，当你不见她时茶饭不思，是激情。老想见到她，想和她一起玩，一起上床，一起做白日梦，是激情。每天短信电话多的没完，是激情。为了生日节日费尽心思想浪漫的点子，是激情。</p>
<p>当你和她在一起时脑子里不自觉的规划实实在在的将来，是爱情。当你们争吵到很凶，火很大时，也不忍心说一句伤害她的话，是爱情。当你们有分歧时，你总是能清楚的知道她是怎么想的，能理解她的初衷，是爱情。</p>
<p>现在很多人谈恋爱很短时间就结婚了，当然，不是说时间短就不好，而是要清楚的知道，婚前的考察对这一生的影响。失败的家庭是任何成功都弥补不了的。</p>
<p>人，一旦离过一次婚，再寻找幸福的难度就更大了。因为，离过婚的人更难相信爱情。更何况离婚对子女的影响，太大太大。人，真正能留在这个世界上的，也就是子女这点血脉而已。为子女营造一个好的成长环境，是父亲的责任。</p>
<p>前面说这么多其实都是说面对婚姻的态度要理性，要慎重。</p>
<p>态度端正以后，技巧也有很多需要注意的地方。明明相爱的两个人互相伤害的事情太多了。如果犯错，也要积极想办法补救争取。伤寒心散伙的也多，而且婚姻生活，男人还有个重要技术要掌握，就是老妈与老婆的关系。婚后生活琐碎的事情太多，先说说处理婆媳关系。所有还没结婚的兄弟一定不要把这不当一回事。一边是生你养你为你付出所有的母亲，一边是将要相伴你一生的老婆。双方都有责任要照顾。这个关系弄顺了，少很多麻烦。</p>
<p>有人说男人在婆媳之间是双面胶，夹在中间两面讨好。这是错的。男人必须是一手拿棒一手拿润滑油。小事居中调节，原则问题对事不对人，道理绝不能歪，既不能纵容媳妇，也不能惯坏了老妈。坚决不要在老妈面前说媳妇不对，多包涵之类，也不能在媳妇面前说，这都是纵容。把握一个原则，绝不允许媳妇在自己面前说老妈的坏话，有事说事。该怎么解决怎么解决，是老妈不对也要和老妈把事情讲清楚，同样的，老妈说自己媳妇的时候也要分清是非。居中讨好的后果是两面不是人，矛盾还越来越深。</p>
<p>总之，绝对的公平是处理婆媳关系的唯一方式，委屈任何一方，都会使矛盾激化。妈也好，老婆也好，底线都给她们画好。人都是有选择性的，既然要不到特权，自然会注意彼此相处。一开始可能男人日子不太好过，两边都要斗上几次。但是慢慢的，家庭秩序就会走上正轨。 如果一开始图省心，哄过去，两边脾气都养大了就够自己受了。（前面忘了说了，找女人一定要找个聪明的，笨女人多很多麻烦，聪明的女人自己会处理，自己只用打打下手，关系就处的很好了。）</p>
<p>还有个问题，就是子女，现在的孩子才真叫一个宝贝，一大家人围着转，各有各的主意，很多矛盾都是因此而起，在这一点上，男人一定要强硬，从一开始就绝对不能让步，要给出明确的信号，孩子是自己的。双方父母肯帮忙，感谢。但是涉及孩子的一切决定。必须是自己拿主意。（这一点背地里可以多听老婆的，毕竟，妈妈是最爱孩子的。）这一点申明尤其重要，相当于自己一个人把所有炮火揽了。否则，孩子一点点小感冒家里可以闹翻天。（我那今天离婚的朋友就是因为这个）。</p>
<p>婚姻确实是个大话题，一时反而不知道说什么好了，我是真心想和大家交流一下。因为我自认为自己的婚姻真的是种幸福。我是真心希望有更多的人能愉快的享受婚姻。有人说不吵架不算夫妻，说真的，我和我老婆真还吵不起来架，就像我前面说的，伤彼此的话确实不忍心讲出口。何况一旦清楚彼此都是出于爱，又有什么好吵的？有几次刚进入状态，看见彼此装腔作势生气的样子就都笑了。</p>
<p>512地震的时候，我在震区，这么大的城市，通讯中断的情况下，我第一时间见到了我的老婆，不在我们上班的附近，也不在我们家附近，而是在我父母的家门口。她知道体谅我担心父母的心情，我知道她能体谅我。这就是婚姻的默契。也是婚姻和恋人的区别，婚姻承载的更多。</p>
<p>恋人时刻只有甜蜜和浪漫，而婚姻则更多是责任和平淡，这个转化的过程，是需要双方有充足思想准备的。其实只要两个人肯一起面对，婚姻生活平淡中的幸福并不输给热恋时的浪漫。</p>
<blockquote>
<p>转载在<a href="https://www.zhihu.com/question/19732277/answer/1056367198">知乎</a></p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[nvidia-smi指令报错：Failed to initialize NVML: Driver解决]]></title>
        <id>https://sunyanhust.github.io/post/nvidia-smi-zhi-ling-bao-cuo-failed-to-initialize-nvml-driver-jie-jue/</id>
        <link href="https://sunyanhust.github.io/post/nvidia-smi-zhi-ling-bao-cuo-failed-to-initialize-nvml-driver-jie-jue/">
        </link>
        <updated>2020-04-05T15:18:44.000Z</updated>
        <content type="html"><![CDATA[<p>最近装深度学习环境的时候遇见的，主要原因是旧的显卡驱动没卸载。</p>
<p>首先需要卸载驱动</p>
<pre><code class="language-shell">sudo apt-get purge nvidia*
</code></pre>
<p>然后重新安装新的驱动即可。</p>
<p>参考文章：https://www.zhihu.com/people/sun-yan-90-29</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[有趣的网站]]></title>
        <id>https://sunyanhust.github.io/post/you-qu-de-wang-zhan/</id>
        <link href="https://sunyanhust.github.io/post/you-qu-de-wang-zhan/">
        </link>
        <updated>2020-04-05T14:42:54.000Z</updated>
        <content type="html"><![CDATA[<h2 id="音乐">📻音乐</h2>
<ul>
<li><a href="http://music.qkhhyiu.cn/">音乐搜索器</a>: 多站合一音乐搜索解决方案</li>
<li><a href="http://guozhivip.com/yinyue/">果汁音乐</a></li>
</ul>
<h2 id="搜索">🔮搜索</h2>
<ul>
<li><a href="https://scholar.chongbuluo.com/">虫部落</a></li>
<li><a href="http://guozhivip.com/so/">果汁搜索</a></li>
<li><a href="https://jikipedia.com/">小鸡词典</a>：网络流行语</li>
<li><a href="https://zh.wikihow.com/%E9%A6%96%E9%A1%B5">wikihow</a>：生活维基百科</li>
</ul>
<h2 id="排行榜">📜排行榜</h2>
<ul>
<li><a href="http://guozhivip.com/rank/">果汁排行榜</a></li>
<li><a href="https://tophub.today/">今日热榜</a></li>
</ul>
<h2 id="其他">📚 其他</h2>
<ul>
<li>
<p><a href="http://guozhivip.com/eat/">今天吃啥呀</a></p>
</li>
<li>
<p><a href="http://www.underseacat.com/fan">云风扇</a>：心静自然凉</p>
</li>
<li>
<p><a href="https://fonts.safe.360.cn/">360查字体</a> ：你的字体能商用吗</p>
</li>
<li>
<p><a href="https://www.gaoding.com/koutu">搞定抠图</a></p>
</li>
<li>
<p><a href="http://www.nows.fun/">毒鸡汤</a></p>
</li>
<li></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[文章汇总：融合BN加速推理、BERT推理加速实践、pytorch C++前端推理模型以及ReZero: 使用加权残差连接加速深度模型收敛]]></title>
        <id>https://sunyanhust.github.io/post/wen-zhang-hui-zong-rong-he-bn-jia-su-tui-li-bert-tui-li-jia-su-shi-jian-pytorch-cqian-duan-tui-li-mo-xing-yi-ji-rezero-shi-yong-jia-quan-can-chai-lian-jie-jia-su-shen-du-mo-xing-shou-lian/</id>
        <link href="https://sunyanhust.github.io/post/wen-zhang-hui-zong-rong-he-bn-jia-su-tui-li-bert-tui-li-jia-su-shi-jian-pytorch-cqian-duan-tui-li-mo-xing-yi-ji-rezero-shi-yong-jia-quan-can-chai-lian-jie-jia-su-shen-du-mo-xing-shou-lian/">
        </link>
        <updated>2020-04-04T14:48:05.000Z</updated>
        <content type="html"><![CDATA[<h2 id="融合bn加速推理">📚融合BN加速推理</h2>
<p>批归一化（Batch Normalization）因其可以加速神经网络训练、使网络训练更稳定，而且还有一定的正则化效果，所以得到了非常广泛的应用。但是，在推理阶段，BN层一般是可以完全融合到前面的卷积层的，而且丝毫不影响性能。<br>
<strong>参考文章</strong>：<a href="https://zhuanlan.zhihu.com/p/120265831">深度学习推理时融合BN,轻松获得约5%的提速</a><br>
<strong>代码</strong>：keras的暂时没有找到，有空可以写写</p>
<h2 id="bert推理加速实践">📚BERT推理加速实践</h2>
<p>主要基于Faster Transformer，<strong>参考文章</strong>：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/89694963">BERT模型推理加速总结</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/91024786">BERT推理加速实践</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/73715272">NVIDIA BERT推理解决方案Faster Transformer开源啦</a></li>
</ol>
<h2 id="pytorch-c前端推理模型">📚pytorch C++前端推理模型</h2>
<p>使用libtorch C++前端来推理复杂模型，可能会用到。<strong>参考文章</strong>：<a href="https://zhuanlan.zhihu.com/p/69421019">嫌python慢？来这里用pytorch C++前端推理模型</a></p>
<h2 id="rezero-使用加权残差连接加速深度模型收敛">📚ReZero: 使用加权残差连接加速深度模型收敛</h2>
<p><strong>论文标题</strong>：ReZero is All You Need: Fast Convergence at Large Depth</p>
<p><strong>论文作者</strong>：Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W. Cottrell, Julian McAuley</p>
<p><strong>论文链接</strong>：https://arxiv.org/abs/2003.04887</p>
<p><strong>代码链接</strong>：https://github.com/majumderb/rezero</p>
<p>简单来说对残差进行了加权并初始化权重为0来加快网络收敛速度。思路比较清晰，可证明也work，具体参考文章<a href="https://zhuanlan.zhihu.com/p/113384612">ReZero: 使用加权残差连接加速深度模型收敛</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[新冠病毒全球大流行：我们缺乏的只是疫苗？]]></title>
        <id>https://sunyanhust.github.io/post/xin-guan-bing-du-quan-qiu-da-liu-xing-wo-men-que-fa-de-zhi-shi-yi-miao/</id>
        <link href="https://sunyanhust.github.io/post/xin-guan-bing-du-quan-qiu-da-liu-xing-wo-men-que-fa-de-zhi-shi-yi-miao/">
        </link>
        <updated>2020-04-04T14:18:07.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>本文转自微信公众号，<a href="https://mp.weixin.qq.com/s?__biz=MjM5MjYxOTQ2NA==&amp;mid=2650202369&amp;idx=1&amp;sn=738db7d0fc8c69f2dbe5b8ad64ae09e5&amp;chksm=bea1cfa689d646b0183608329abdbf11c53f64924ce476a219bc39cbb7373805c3dbaf36c936&amp;mpshare=1&amp;scene=23&amp;srcid=0404YwhJm01ENyZGtvQaeyr6&amp;sharer_sharetime=1586009335106&amp;sharer_shareid=47825813c3bfc95e426cc37b214c1ac0#rd">原文</a>。个人觉得写得非常好，作为此次疫情的反思和总结。</p>
</blockquote>
<p>又一次的开学典礼付诸东流，这一次春天的典礼是因为新冠肺炎，上一次冬天的典礼是因为香港动荡。不少朋友问我怎么没有看到我在新冠肺炎下的演讲、观点和文章？近两个月里，好文何止上百上千？有多少从疫情中央发出的令人潸然泪下的亲身经历？有多少发自内心的自省和思考？有多少对国家未来的焦虑和期许？我们已经好久没有经历过这样的场面，在同一个时刻、为同一个人、为同一件事发出我们谦卑的声音，吹起我们的口哨声？而这都是为了同一个目标，希望类似的悲剧可以再少些；希望我们无需生活在不必要的恐惧之中；希望这个民族无论何时都是被人敬重的。</p>
<p>这当然是一场灾难。庚子鼠年以超出所有人的想象力，开始了这一场天灾，但这也是一场人祸。根据英国南安普敦大学的研究，如果武汉提前三个星期开始狙击这一病毒，仅中国受感染的数目就可以减少95%。当然这只是一项研究，而现实与数字模型之间存在的距离有时可以是如此之大！如果武汉封城之后欧美各国不会如此傲慢，而是积极合作应对，今天的欧洲和美国或许就不会面对这样的人道危机！在全球面临这样的大灾难面前，相反，我们看到的是自私与自大、嘲讽与指责、恐惧与推卸，甚至阴谋论甚嚣尘上，代替了理性的思考和应有的反思。面对这样的世纪疫情大流行的恐惧，我们缺乏的远非控制疫情的疫苗！</p>
<p>我们缺乏常识；我们缺乏见识；我们缺乏透明度；我们缺乏同理心；我们缺乏担当；我们缺乏反思……</p>
<h2 id="我们缺乏常识">我们缺乏常识</h2>
<p>在这场疫情席卷全球时，新冠肺炎也成了阴谋论的温床。短短的一个多月时间里，有武汉病毒研究所病毒外漏的“泄毒之说”，有美国驻武汉领事馆留下八个可疑生化毒物箱的“种毒之说”，有美国参议员柯顿指控毒源来自大陆生化实验室的“放毒之说”，有武汉军运会期间美国兵“播毒之说”，莫衷一是。我从一开始就对阴谋论存疑，我总觉得人性虽恶，但人类的恶行还不至于如此匪夷所思。有些指控，稍微求证，就知道是胡言乱语。美国驻武汉总领事馆位于武汉新世界国贸大楼第47楼，后院在哪里？生化毒物箱又如何埋在地下1.5公尺处？谎言哗众取宠，但信者众！如果病毒来自中国的生物基因作战实验室，对病毒的认识和控制还会那么难吗？这样低水准的阴谋论竟然畅行全球！其实只要有基本常识，反智的阴谋论就不可能大行其道。</p>
<p>我们缺乏常识也因为我们常常以偏盖全，信息不对称。意大利专家雷穆齐（Giuseppe Remuzzi）表示早在去年11月份，意大利北部就有人染上高度疑似新冠肺炎的不明肺炎。中国的一些媒体第一时间就报道了意大利是源头的说法，让不少中国人信以为真。中国的记者还纷纷打电话去采访，他对自己早先的采访被断章取义非常不满，并指出这是教科书式的“宣传手段”。但之后他纠正中国媒体的说法，并没有被广泛报道。他还在另一个场合表示，武汉可能早已出现新冠肺炎感染，期间有大量中国人从武汉来到意大利，令意大利出现了疑似案例，由于一切来自中国的信息都不透明，才令疫情失去控制的黄金时机。其实意大利北部温州的人很多，而一月的时候，除了武汉，温州疫情也很严重。在中国就有专家因看到浙江有人感染之后，强烈建议武汉必须封城。</p>
<p>疫情刚爆发时，有不少人总在那里质问，美国每年季节性流感死了成千上万人，无人恐慌，世界各国没有切断和美国的联系，但美国为何要切掉和中国的联系？这是否过度反应？是否歧视中国？是否违反世界卫生组织的指引？但季节性流感有疫苗，死亡率只有新冠肺炎的十分之一，这样的事实很多人并不了解。如今看到疫情蔓延全球，纽约成为另一个武汉，大家应该可以明白每年在美国发生的季节性流感和新冠肺炎之间的根本不同。</p>
<p>我们缺乏常识是因为我们受制于我们有限的知识和见识，无法认识事物的真相；我们缺乏常识也是因为我们面对恐惧而惊慌失措，无法理性地看待自媒体时代所获得的虚假资讯；我们缺乏常识也是因为我们的立场和偏见挡住了自己的视线，无法走出原有的认知。</p>
<h2 id="我们缺乏见识">我们缺乏见识</h2>
<p>同样在欧美各国，不少人还真的将新冠肺炎和季节性流感等同起来，根本不把新冠病毒当回事，酿成今日欧美各国沦陷的惨痛教训。在发生新冠肺炎这样的全球公共医疗危机时，不要说普通人，即便全球最顶尖的传染病专家对病毒都缺乏足够的认识，束手无策，无法预见其发展方向，至多只能依靠模型做出推算，但最终和现实也可能相距甚远。在疫情初期，不要说西方的专家，即便内地最顶尖的传染病专家都不认为这次疫情比“非典”严重。因香港的特殊地位，香港大学的专家学者敢于发声，袁国勇教授早在1月3日就警告香港政府，这次疫情极为严重，香港特区政府早在1月7日就宣布把“严重新型传染性病原体呼吸系统病”列为须呈报的疾病，卫生部门有权强制隔离怀疑患者。管轶教授是香港大学新发传染性疾病国家重点实验室主任，最早发出疫情将失控的警告。</p>
<p>香港因2003年受“非典”的沉重打击，大家记忆犹新，不少香港人对新冠肺炎都非常恐惧，也出现了抢购潮。但在香港的西方人对此的反应就大为不同，包括香港大学中的白人学者也觉得这是类似流感的病毒，只不过传染率和死亡率高而已。这种判断一度让我觉得香港是否过度恐慌了，特别是香港医务人员以罢工逼迫政府封关的举动过激，违背了医务人员救死扶伤的伦理底线。疫情初期在香港街上也基本看不到有多少西方人戴口罩，所以在西方的华人因为担心感染病毒戴口罩也被视为怪物，不被理解还算次要，还受到白眼和歧视，甚至遭人毒打。因为在西方的文化里，只有得病的人才戴口罩，而你得病了就不该出现在公众的地方。其实西方人这样的行为也是因认知受限，而诉诸暴力的不法之徒更是蔑视人权。</p>
<p>这次疫情在欧美的迅速蔓延终于让西方意识到新冠病毒不只属于亚洲人，他们原先冷眼旁观，以为白种人可以刀枪不入。甚至欧美的不少医学专家初期都低估了这个疫情的风险，从意大利大意失荆州，到英国的“群体免疫”，再到美国的全线沦陷，在一定程度上都和他们对这个病毒的有限认知有关，说难听点就是无知。因此政府不敢与普通民众的认知相左，轻易做出封城的决定。医学界本身也存在完全对立的看法，直到伦敦帝国理工学院流行病专家尼尔·弗格森团队的研究报告做出了令人恐惧的预测之后，英美两国政府才改变被动的应对策略。这份研究报告警告，如果英美两国不积极应对，英国将会有超过50万的人死亡，而美国将有220万人死亡。即便如此，牛津大学的研究团队在此之后还是得出截然不同的结论，认为新冠病毒在英国已经传播了一个多月，大约一半人口已经获得了实质的群体免疫能力。</p>
<p>全球在应对这个新型病毒的侵袭时，因为知识不足，做出了不少错误的判断。在疫情初期，对病毒的严重性难以做出正确的判断，在获得人传人的证据之后才被迫做出武汉封城的决定。但同时也因为认知不足，人的见识有限，影响了我们应对病毒的策略。东亚地区有“非典”的惨重教训就极为重视，西方民众对新冠病毒的认知则不同，完全放任。</p>
<p>我们的见识常常受限于我们的生活经历和环境，但我们不可能亲临其境去认知每一件事物，因此获取全面的信息就变得至关重要。不幸的是我们因防火墙无法获得客观的信息，因处在同温层里拒绝不同的信息，更不要说我们因缺乏透明度难以接收真实的信息。</p>
<h2 id="我们缺乏透明度">我们缺乏透明度</h2>
<p>回首往事，不少人都低估了疫情的危害！但是，在疫情还没有开始蔓延时，如果做到信息公开透明，如果吹哨人不被劝诫、警告、和惩罚，或许新冠肺炎全球大流行的历史会改写。许多在武汉不该发生的事一定不会发生，武汉的牺牲就不会这么大，中国百姓的牺牲也就不会这么大。</p>
<p>从中国最早处理新冠肺炎的不当做法，到世界卫生组织迟迟未对全球发出最高级别的警告，到欧美各国的迟缓应对行动，都和缺乏透明度有关联。这次疫情如此迅猛扩散的第一责任人当然是武汉当局、湖北当局，他们对公众隐瞒信息甚至掩盖真相，引发了民众的不信任，国际社会不少人甚至怀疑中国的死亡率造假。中国最为受伤的就是因封锁和隐瞒信息，导致疫情的控制受到延误，遭到国际社会诟病、排斥和指责。武汉封城之后，中国的经济和民生受到重创的举措和牺牲也因此大打折扣，对中国的负面影响其实刚刚浮现。</p>
<p>美国政客在中国疫情最严重的时刻，颇有隔岸观火的看客心态。特朗普为了选举，为了股市不下跌，不影响经济，就是不愿承认疫情迟早会冲击美国。他本以为关闭了来往中国的航线，切断了来自中国的人流就万事大吉了。他还不让邮轮上受感染的游客在美国下船，就是要制造美国本土病毒感染者很低的假象。但这样的做法和一切以稳定为首要的考虑有何区别呢？在疫情终于席卷美国之后，他也是不断大事化小，尽量降低疫情所带来的冲击和影响，甚至在感染人数还在不断攀升时竟然表示美国的经济活动在复活节就可以恢复正常！所幸美国有独立的媒体，在白宫可以直接和总统公开叫板，不让政府传播的不实消息当道。在白宫记者会上，美国媒体公开质疑特朗普的抗疫政策不当，当场质问总统为何不停地使用“中国病毒”这样的歧视性字眼。</p>
<p>早在1月20日，当我确认这将是一场公共卫生灾难时，我就第一时间在我的朋友圈里转发了管轶教授对疫情的“悲观”看法。但他的科学分析在内地被视为耸人听闻，有人甚至借他的“逃跑说”对他进行人身攻击，但正是这样客观的信息才有助于我们了解事实真相，了解这一公共卫生危机已经去到了多么危险的境地！其实在发生类似新冠肺炎这样的危机时，面对太多的不确定性，要阻止谣言，信息的透明就显得尤为重要。</p>
<p>除了刻意隐瞒信息，还有虚假资讯泛滥。全球数百名科学家2月上旬出席日内瓦“世卫论坛”，讨论新型冠肺炎疫情，学者就感叹他们不得不面对两条战线作战，除了应付病毒大流行，还要应付虚假资讯大泛滥 ，而应对虚假资讯泛滥比抗疫本身还艰难。网上流传最广最快的往往就是耸人听闻的假消息和渲染成见的看法，这些不实的信息，有恶意造谣，有断章取义，导致非理性的反应和恐慌，甚至制造混乱和分化。世卫顾问隆基尼（Ira Longini）和香港大学医学院院长梁卓伟曾提及全球三分之二的人口有可能感染新冠病毒，但网上的信息都忽略了“如果传播未加抑制”的假设，特意将最坏的可能性无限放大，引起不必要的恐慌。</p>
<p>在任何一场公共危机发生的时候，政府是不可能靠屏蔽信息来阻止危机的蔓延。恰恰相反，这只会造成危机的进一步恶化。即便在上个世纪的苏联时代，对切尔诺贝利核泄露的隐瞒最终给人类带来了一场世纪大灾难，更何况我们已经身处社交媒体如此发达的时代！</p>
<p>面对全球疫情大流行，信息披露和信息对称有助于我们了解不同地域，在不同的文化和背景下的不同应对策略和措施。不管是对疫情的判断，还是应对疫情的方法，各国都有不同的理解和做法，相互之间不仅不该嘲笑，反而应该借鉴。我们因条件限制无法获得全面的信息，但至少可以换位思考，从他者的角度看问题，避免幸灾乐祸的看客心理。</p>
<h2 id="我们缺乏同理心">我们缺乏同理心</h2>
<p>疫情爆发之后，各国不仅有不同的认知过程，而且在获得相同的认知之后所采取的应对也并不相同。武汉封城的消息传出之后，西方的反应也是两极，有称这样的举措是流行病专家的天堂，而这只有在威权国家才能实现，民主国家只能羡慕。但也有一些西方国家看到中国面临的困境，在疫情刚刚爆发时，也带有事不关己高高挂起、甚至幸灾乐祸看笑话的心态来看待中国的抗疫，还把病毒与中国的国民性和低劣文化相联系。</p>
<p>各国抗疫的做法离不开其体制、文化、历史等因素。在中国，一声令下，举国体制立马见效，整个国家有如一部机器，全力抗灾，所有其它事情都要靠边站，甚至做出牺牲，包括在“准战争”状态下个体的权利和自由，其它病人可否受到正常的医疗救助，都不是最重要的考虑。事实证明，这样的牺牲确实巨大，但这一抗疫历史上未曾经历过的举措，一座上千万人口的大城市被封城两个月的战略最终是奏效的。</p>
<p>中国的牺牲阻止了疫情蔓延，可歌可泣。即便如此，可圈可点之处也多如牛毛，野蛮作业的现象也并非个别。中国人不喜欢美国指手画脚，那别的国家难道就喜欢中国这么做？一些自媒体对别国状况一知半解，充满无知、偏见和轻蔑，非要说人家不会抄作业。看看东邻日本，和韩国的做法也不同，连大面积的检测也没做，情况也不算太坏！日本的人口密度还超过中国！但日本人平时的生活和卫生习惯，你又了解多少？其实就是华人社会的香港、澳门、台湾、新加坡等地的处理方式都不同，当中新加坡的所谓“佛系”防疫措施相当成功让不少人大跌眼镜。</p>
<p>新加坡从“重灾区”到“模范生”，表面上看去似乎选择了“佛系”的抗疫策略，曾引来不少怀疑、甚至嘲笑。新加坡防疫成功是有原因的，其策略可以概括为：最快反应、最早防范、最有系统、最严惩罚、最少折腾、最缺恐慌。新加坡一度是仅次于中国病例第二高的国家，同时人口稠密，还是国际交通枢纽。但新加坡政府反应迅速且效率高，最早限制来自中国的人流，并实施了对不同人群的休假令和居家隔离令。“非典”之后建立起来的疫情警报系统立即派上用场。新加坡国家传染病中心集先进的检测、治疗与实验研究为一体，马上研发并合作生产了快速病毒检试剂，有健全的检测体系，保证了疑似患者尽快得到治疗，避免了疫情的传播，加强了民众的信心。新加坡缺乏口罩生产能力，不鼓励大家戴口罩，但政府还是快速购买了五百万个口罩派发到每家每户，安抚民众。新加坡有充足的医疗资源，类似于中国的发热门诊就有873个，相当于北京发热门诊的11倍。我很早在朋友圈里就转发相关的信息看好新加坡的做法，甚至比香港还成功，没有发生香港排长队争口罩、抢厕纸的“奇观”。但话说回来，香港的恐慌是基于香港曾在2003年“非典”时曾遭重创的惨痛历史，以及香港和内地每天有大量的人员来往这一事实。</p>
<p>韩国这次的抗疫模式在西方更是受到肯定，法国总统和瑞典首相等多国政要甚至致电韩国讨教。但韩国对疫情的控制到底有何魅力？为何西方愿意到韩国取经和复制韩国模式呢？韩国也曾面对与中国相同的困境，但两国在大范围发生疫情之后，采取了类似的抗疫战略，新增病例曲线迅速被压平。但西方在看韩国的经验时，特别看重韩国没有因疫情出现压制言论和信息受阻的现象，没有因禁令影响民众的行动和自由，国家的经济更没有受到太大的冲击。韩国的经验可以归结为：早干预、早准备、早检测、早跟踪、早隔离、早观察。韩国的企业早就判断病毒迟早会扩散到韩国，第一时间就研发出检测试剂盒，获得政府的紧急审批投放市场，检测过程只需十分钟，几小时内可以出结果，准确率超过98%。韩国单日可检测近两万人，检测率全球之冠，已有120多个国家争相从韩国进口测试盒。韩国政府还迅速修订法律，网站和手机都可以追踪病发者，一旦有新病例，就可以获得信息和警报。</p>
<p>好的经验当然可以抄，可以借鉴，但不必过分地显耀自己的成功，这只会让人反感。己所不欲，勿施于人。现在中国不准外国人入境，这是因为中国不能再冒第二次疫情失控的风险，于情于理都不是自私自利。同样，疫情爆发初期，香港、新加坡、意大利、美国等地对中国人封关、撤侨也是同理，人家也同样不愿意看到疫情蔓延，为何那时就可以攻击别人是恶意制造恐慌，是对中国背后插上一刀呢？美国在欧洲疫情严重之后也禁止欧洲人前往美国，最后连英国这个小兄弟也进了入境限制名单。日本现在对包括中国、韩国、美国、欧洲在内的国民入境都采取十四日隔离的政策。疫情初期，中国民众对日本的态度发生了180度的大转弯，曾经被我们骂得一无是处的大和民族似乎对中国很友好、很善良，向中国捐赠各类物资，而对美国政府的表现极为不满。其实抛开美国民间和企业的资助不提，为何一定就要期待和中国正在打贸易战的特朗普政府对你友好呢？而对中国最早锁国的是朝鲜、俄罗斯、越南等国！</p>
<p>在疫情袭击的恐惧中，我们更不可以幸灾乐祸地嘲笑别人的行为，透过渲染别国的疫情失控来展现自己的英明和伟大，而忘记了自己并没有走出险境。美国和意大利的报纸上密密麻麻的讣告，看去令人悲伤和沉重，恰恰彰显了人性的一面。中国不少媒体将意大利和美国医院中的尸体的照片无限渲染，而失去亲人的武汉人前去领取骨灰盒，为了正常的悼念发出的哀思和照片却消失了。我们当中总有人不愿正视自己的创伤，不可忍受将苦难、悲剧和丑恶呈现在他们面前的人，将读者高达五千万的“日记”视为恶毒、无耻，却又如此钟情地展示“纽约医院尸满为患”、“纽约穷人疫情之下被迫乘坐地铁上班”、“英国政府勒令医生封口”这样的文字和照片。广东一个企业老板竟然建议厂家做假测温枪卖给美国，让感染者越来越多，辽宁有餐厅门外贴出横幅祝贺美日疫情扩散，就不单单是没有同理心了，而是无知的反人类言论。</p>
<p>如果我们可以同样毫无顾虑地拷问自己，犹如如此心安理得地对他人提出质疑，我们的心智就一定不会萎缩，我们兴许也就有了希望。如今，我们甚至无法正常地伸出舌头，道出自己的甜酸苦辣，又何必如此居高临下，带着幸灾乐祸的病态，刻意营造似是而非的场景，来彰显那虚幻的优越感？！但我总是固执地坚信，一个人、一个国家、一个民族只要勇于承担起苦难中的责任，最终一定是会得到别人的理解和赞许的。</p>
<h2 id="我们缺乏担当">我们缺乏担当</h2>
<p>在这次疫情中最常听到的一个字就是甩锅，这场“甩锅大战”从武汉封城的那一刻开始就不断上演，从当地的医疗机构，到各级政府官员，到中国疾病控制中心，大家都在问，疫情失控和蔓延的责任在谁？</p>
<p>中国在“非典”之后耗资11亿，搭建了全球最大的传染病疫情和突发公共卫生事件网络直报系统，过去15年间持续监测39种法定传染病。这个全球最快速的疫情上报系统，可以在短短两小时内将疫情上达北京，中国最高的疾病防疫专家在2019年曾经表示中国绝不会重演“非典”悲剧。但话音刚落，这个耗费巨资的系统并没有在这次病毒蔓延中发挥功效。或许我们永远都无法知道真相，但有一点很清楚，专业判断在明哲保身、没有承担的官僚系统中被冷冻了，生命的价值也同样在个人权力的棋盘上被抛之脑后。</p>
<p>这场疫情最大的讽刺是，全球最大的两个经济体在面对这场世界公共卫生大危机时，竟然上演了一出极为相似的闹剧。几乎每天陪同特朗普在白宫见记者的美国传染病首席专家福西不谄媚权贵，不介意道出与他旁边的总统立场不同的看法，其独立的专业精神不受政治的左右，但他的专业判断也同样被美国总统束之高阁。疫情在中国蔓延恶化之时，美国的科学家就发出警告，但美国疾病防疫中心、美国食品和药物管理局、美国卫生和公共服务部似乎都没有看到采取行动的紧迫性，更何况美国总统特朗普本人了。特朗普向来蔑视科学和专业的意见，联邦政府被一群科学怀疑论者把持。而特朗普就喜欢看极右的福克斯电视台，曾与中国同行舌战的女主播Trish Regan就鼓吹疫情是民主党的阴谋，而特朗普本人就是一个阴谋论者。他同样不信任主流媒体，不停地和主流媒体在白宫记者会上唇枪舌剑，甚至当众侮辱记者。特朗普也不重视来自情报机关的报告，警告疫情的严重性被中国低估和隐瞒，以及疫情将会蔓延全球。此外，特朗普对玩政治的兴趣多过抗疫，为了竞选就是不愿承认疫情迟早会冲击美国，他对疫情轻描淡写的原因也是因为民主党主政的纽约州、加州、华盛顿州受到重创，但共和党的红州并未受到太大的影响。纽约时报在3月28日刊登万字文，以“美国错失的一个月”为题，分析了美国因检测技术落后，法规不配套，白宫领导无方，政府官僚作风，导致美国失去了疫情防控的黄金30天。</p>
<p>美国的科技和医疗发达，美国的医疗开支占GDP的比例最高，达到了近18%，但美国至今的表现为何令人大跌眼镜？无法早期进行检测是疫情蔓延的元凶，美国疾病防控中心也不是不作为，但为何会发生这样灾难性的失误呢？这和欧美社会对新冠肺炎的轻视有相当大的关联。中国在修正了前期隐瞒疫情的错误之后，武汉封城的快速行动，为整个国际社会控制疫情争取了难得的宝贵时间。随后东亚各国和地区也纷纷采取行动，大体上都取得一定的成效，制止了新冠病毒的蔓延。遗憾的是，由于对疫情的认知存在极大的偏差，欧美国家都没有及时采取适当的应对措施，欧洲和美国先后演变成疫情的重灾区。此次疫情的另一个中心意大利，也只不过停飞了前往中国的航班。而美国早在1月3日就获得了中国的通报， 但美国和其它欧洲国家一样一直心态超然，觉得自己远隔重洋，“非典”只在东亚流行，便以为此次新冠肺炎也同样会局限在东亚地区。</p>
<p>而疫情在美国开始蔓延后，这场“甩锅”大战竟然也蔓延到国际社会，中美两国爆发了令人捧腹的唇枪舌战。中国外交部的新任发言人在推特上怀疑美军在武汉播毒，特朗普亲自上阵，恶意地称新冠肺炎为“中国病毒”。病毒起源地的争论凸显了各方意图透过“甩锅”来推卸应有的责任，其实起源地何罪之有？而美国国务卿蓬佩奥在特朗普改口之后，还坚持要将武汉病毒写进七大工业国外长的公报里，而被其它国家拒绝。美国自己浪费了一个多月的时间，疫情失控，特朗普却只会将矛头转移，掩盖自己抗疫能力的失误！更为严重的是，“中国病毒”经过他的大嘴巴，在推特里一天又一天地在说，传遍全球，造成了美国等地歧视亚裔人的犯罪上升。美国联邦调查局的一项全新研究，警告全美针对亚裔人的仇恨犯罪案件数目，因新型冠状病毒疫情的扩散而飙升，危及美国的亚裔社群。连新加坡总理李显龙在接受美国有线电视新闻网CNN采访的时候，不仅感慨美国失去了领导世界战疫的能力，而且惊叹这两个世界大国竟然可以如此低水平地进行“口水战”。</p>
<p>从亚洲到欧洲到美洲，昔日繁忙的大都会因这场疫情，生活已经停顿。这场疫情不仅暴露了我们制度的缺陷、系统的脆弱、和人性的罪恶，全球已经跌入新一轮的金融市场大动荡和全球经济大衰退，但不幸的是，我们不仅没有进行反思，却依旧在那里自我陶醉和自我撕裂！</p>
<h2 id="我们缺乏反思">我们缺乏反思</h2>
<p>一场史无前例的病毒大流行正向全球各个角落冲撞，死亡笼罩着这个星球。但面对这场突如其来的天灾，其中多少人祸是可以避免的呢？</p>
<p>封口vs封城：围绕着这场人道危机的争论焦点从一开始就从这里展开。如果没有发生“封口”事件，新冠肺炎的蔓延是否会有另一个结局？我们无法知道答案，但我们知道至少不会如此惨烈。问题在于一个经济如此发达的国度，为何依旧无法实现一个开放社会所需要的基本条件；一个自信的社会为何难以拉响危机来临的警报声。而这并非个别和单一现象，这有如隐藏在我们社会中的毒瘤和顽疾，总是如此粗暴地压制善意的提醒和批评。</p>
<p>在危机抵达临界点之后义无反顾的“封城”行动，尽管惨烈，却也是迫不得已的孤注一掷，但我们并非事事都一定要以牺牲个体的代价来实现宏大的目标，文明是体现在对每一个生命的关怀上的。“封口”可以令一个民族、一个国家在全球失去信用和信任，即便在“封城”的巨大牺牲之后，受感染和死亡的官方的数据还是被质疑。扪心自问，为何中国常常成为这类被怀疑的目标与对象？一个真正开放的社会，和一个透明度高的社会，一定可以勇敢地面对真相并向大众提供真相。所幸，在疫情重击下，中国也出现了难得一见的媒体松绑现象。</p>
<p>另一方面，西方也常常从固有的认知出发，用有色眼镜看待中国的“封城”行动。在这场抗疫中，与东亚各地在武汉“封城”之后迅速进入作战状态完全不同，欧美各国不仅负面看待中国的“封城”行动，而且没有从中国的“封城”行动中嗅出危机的严重程度。</p>
<p>傲慢vs自大：这让我们再次活生生地看到了傲慢与无知，欧美各国普遍将最初在武汉出现的新病毒归结为黄种人的病。日本副首相兼财务大臣麻生太郎2月份曾在G20财长的一次会议上主动表示援助意大利和西班牙，却自讨没趣，欧洲国家非常不屑。意大利副总理后来在G7财政会议上更直截了当地表示，这是黄种人才会得的病，和他们西方人没有关系。无怪乎，意大利一度成为中国之外感染者最多的国家。特朗普的傲慢与自大终于在疫情横扫美国之后，被迫承认美国将面对比第二次世界大战还要惨重的死亡。</p>
<p>然而与西方的傲慢相对应的则是在中国自媒体的世界里无时不在的自大，在那里你只有看到中国成了全球抗疫的英雄和救世主，所有的悲剧都活脱脱地变成了赞歌的素材，而忘记了病毒是从武汉开始向全国和全球蔓延的。这样的自大在中国抗疫初现曙光之后，更是变成了对他国肆无忌惮的嘲笑。而最新的对象就是感染新冠肺炎人数最多的美国，却忘记了美国拥有强大的科技力量和发达的医疗体系，仅ICU（重症监护室）的床位数量就远远超过中国。而“傲慢”与“自大”这对孪生兄弟却拥有一个共同点：偏见。</p>
<p>吃野味vs戴口罩：在有关病毒源头的吃野味文化，以及防止病毒扩散的戴口罩文化的讨论中，我们也看到了类似的偏见。2003年“非典”之后中国人的确没有从中吸取惨痛的教训，及时关闭野味市场，不少人因而将此次病毒的爆发与中国人喜爱吃野味的文化联系在一起。这样的看法有其道理，中国人是时候改变吃野味的生活习俗。有趣的是，中国网民反而找出了纽约上流社会吃野味的视频，一时之间在朋友圈中疯传，证明美国人不过是五十步笑百步。不过这几年比较严重的传染病，包括中东呼吸综合症和甲型H1N1流感病毒并非源自中国。</p>
<p>另一方面，亚洲人戴口罩以防止病毒扩散基本是共识。但西方人，即便是生活在亚洲的西方人也不喜欢戴口罩。在西方，视口罩为病人标志的观念还带来了对亚裔人的歧视。在欧美各国生活的亚裔人处在戴口罩被歧视，不带口罩怕染上病毒的天人交战中。但在这次疫情重击欧美之后，戴口罩抵抗病毒的认知终于慢慢开始在欧美被接受了。</p>
<p>自媒体vs主流媒体（赞美vs批评）：在疫情的报道上，中国的主力军是自媒体，不管是赞歌，还是批评，自媒体带有更多的主观性和情绪性。而在许多其它地方，报道疫情的主力是主流媒体，力求客观。特别是美国媒体，其角色是监督政府，且喜欢监督全世界的政府，多数又是自由主义倾向，所以特朗普也反感美国主流媒体。但只有在美国这个国家，CBS记者胆敢在白宫怒怼总统为何要使用歧视性的“中国病毒”；NBC的记者质问特朗普吹捧效果未经证实的抗疫药物是否给美国人虚假的希望，并指美国数百万人活于恐惧中；纽约时报驻京记者张彦（Ian Johnson ）的“观点”文章，指出中国为美国赢得了时间却被美国白白浪费了；纽约时报的社论公开谴责特朗普政府官员的言词加剧了对亚裔的种族仇恨。</p>
<p>中国自媒体里那个发自纽约的抗疫日记，作者声称其素材全部取自美国媒体的公开报道，而非道听途说，语带双关。的确，当纽约成为美国的武汉时，我每天在美国电视新闻上看到的几乎全是“负面”消息。每一个活生生的人离开人世时的凄惨故事；病人因缺乏医疗设备无法获得及时医治的悲剧；医务人员面对死亡威胁战斗在第一线几乎崩溃的场面；停留在街边装满尸体的冰冻车和医院走廊里运尸袋的场景；质问白宫何时可以确保医疗设备运抵现场的愤怒；受到病毒感染威胁下美国海军官兵的呼吁；失去工作的普通人无法交付房贷的忧虑。在这里你看到的是恐惧，是担忧，是悲伤，在这里你听不到任何赞歌。</p>
<p>威权vs民主：这次全球抗疫的叙事已经成为中国模式和西方模式之争，甚至上升到威权还是民主体制在抗疫中哪个更有成效的争论，但不少人却忘记了无论何种体制都有其成功与失败的经验与教训。在欧洲成为重灾区之后，德国的死亡率却一直很低，这或许与日耳曼民族的自律有关。在亚洲处于恐慌的时候，日本并没有跟随中国封城、没有跟随韩国大面积检测，但也没有像欧美发达国家那样失控，这或许与大和民族的自律和生活习惯有关。如果将抗疫简单地看成是中国体制的胜利，那么韩国、日本、新加坡、香港、台湾等地又是何种体制？无疑，中国自上而下的动员力量，让全球看到了中国体制战胜疫情的超强能力，但自下而上的公民社会的应变和调整能力在纽约成为疫情重灾区之后，同样令人刮目相看。</p>
<p>纽约在中央公园、体育场迅速建起方舟医院，并加快对受感染疑似人员的检测。来自全美的六万多名医务人员主动报名成为自愿者，自发前往纽约支援人手不足的医院，“捷蓝”航空免费载送这些医护人员“上战场”，酒店免费提供住宿，企业慷慨捐赠急需的防护用品和医疗设备，但没有企业对这些行动发稿、做公关、高调宣传。即便美国总统面对新冠肺炎的反应丑态百出，但这个国家所幸不是一个人说的算，受疫情影响最大的纽约州、加州、华盛顿州都不理睬他的狂言妄语。而美国的体制也决定了联邦政府对州一级政府的事务不可干涉，即便特朗普想对纽约和临近的两个州“封城”，但纽约州州长公开反对，使得特朗普不得不放弃这一想法。特朗普随心所欲，疫情还未控制，就要求复活节恢复经济运作，但疾病专家和媒体则公开和他唱对台戏。因此，应对病毒需要在一个自下而上的公民社会里，民众敢于承担公民应有的责任和义务。</p>
<p>在全球面对这场前所未有的大灾难时，我们需要理性地思考人类的失误和失败，而非指责和推卸。这场大灾难离落幕之日还有漫漫长路，但这场天灾与人祸也给人类提供了一次难得的反思机会。在这场疫情结束之后，或许全球终将明白这不是“中国病毒”，是各国必须共同面对的“世界病毒”，病毒恰恰因我们人类的傲慢、自大、和自私而四处肆虐。在这个全球化被污名化的时代，尽管国与国的界线依旧分明，但病毒绝不会只在一国的边境线内停留。我们比以往任何时候都更需要有全球的视野和全球的胸怀，我们必须学会如何合作去共同应对前所未有的挑战。</p>
<p>庚子年常常是灾难之年，但或许也是转折之年。在新冠病毒横扫全球之后，这不应该是我们重拾孤立的时刻，而是通向一个不一样的全球化新时代的新起点。</p>
]]></content>
    </entry>
</feed>